{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72137fd8",
   "metadata": {},
   "source": [
    "# Rank‑Based Diffusion | Facebook Pages (Top 2 000)\n",
    "*Proof‑of‑concept analysis notebook — generated 2025‑07‑24*\n",
    "\n",
    "This notebook implements the workflow defined in **Prompt v0.2**:\n",
    "\n",
    "1. Data QC and pseudo‑share construction  \n",
    "2. Weekday‑aware 7‑day transition matrices  \n",
    "3. Parameter estimation (σ, κ, η, β̂)  \n",
    "4. Euler–Maruyama simulation & diagnostics  \n",
    "5. Artefact export (`results/`)\n",
    "\n",
    "> **Re‑run instructions**  \n",
    "> • Requires Python 3.11 + `numpy`, `pandas`, `duckdb`, `pyarrow`, `matplotlib`, `scipy`.  \n",
    "> • Set the variable `PROJECT_ROOT` below to your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1dd40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1 | Setup & imports  (safe for headless / non-GUI environments)\n",
    "################################################################################\n",
    "import os, sys, json, math, itertools, gzip, datetime, pathlib, warnings\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib       \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = pathlib.Path(\"/Users/hindman/data/rank-diffusion/\")               # ← change if needed\n",
    "DATA_PROMPT   = PROJECT_ROOT/\"fb_top2000_ranked_daily.parquet\"\n",
    "DATA_UPLOAD   = pathlib.Path(\"/Users/hindman/data/rank-diffusion/fb_top2000_ranked_daily.parquet\")\n",
    "\n",
    "RESULTS_DIR   = PROJECT_ROOT/\"results\"\n",
    "FIG_DIR       = RESULTS_DIR/\"figures\"\n",
    "TABLE_DIR     = RESULTS_DIR/\"tables\"\n",
    "MATRIX_DIR    = RESULTS_DIR/\"matrices\"\n",
    "\n",
    "for p in [FIG_DIR, TABLE_DIR, MATRIX_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db54dd",
   "metadata": {},
   "source": [
    "## 2 | Load daily panel & basic QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "672722e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                 endpoint_id  metric_value  rank  weekday\n",
      "0 2020-10-27                    Fox News       2885276     1  Tuesday\n",
      "1 2020-10-27                 Dan Bongino       1913939     2  Tuesday\n",
      "2 2020-10-27                   Breitbart       1755699     3  Tuesday\n",
      "3 2020-10-27                    LADbible       1367933     4  Tuesday\n",
      "4 2020-10-27  Donald Trump For President       1230621     5  Tuesday\n",
      "Loaded 2,289,797 rows spanning 2020-10-27 00:00:00 ▸ 2024-03-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def load_panel() -> pd.DataFrame:\n",
    "    \"\"\"Read the Parquet file via DuckDB streaming; return a pandas DF.\"\"\"\n",
    "    path = DATA_PROMPT if DATA_PROMPT.exists() else DATA_UPLOAD\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\"Parquet file not found in either location.\")\n",
    "    con = ddb.connect()\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT date, endpoint_id, metric_value, rank\n",
    "        FROM parquet_scan('{path}')\n",
    "    \"\"\").fetch_df()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "    df[\"weekday\"] = df[\"date\"].dt.day_name()\n",
    "    return df\n",
    "\n",
    "df = load_panel()\n",
    "print(df.head())\n",
    "print(f\"Loaded {len(df):,} rows spanning {df['date'].min()} ▸ {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81201a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing calendar dates: 73 (listed if ≤10)\n",
      "[datetime.date(2021, 10, 18), datetime.date(2022, 8, 4), datetime.date(2022, 8, 10), datetime.date(2022, 8, 13), datetime.date(2022, 8, 15), datetime.date(2022, 8, 19), datetime.date(2022, 8, 28), datetime.date(2022, 9, 5), datetime.date(2022, 9, 9), datetime.date(2022, 9, 11)]\n"
     ]
    }
   ],
   "source": [
    "# Quick missing‑date check\n",
    "all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"D\")\n",
    "missing   = sorted(set(all_dates.date) - set(df[\"date\"].dt.date))\n",
    "print(f\"Missing calendar dates: {len(missing)} (listed if ≤10)\")\n",
    "print(missing[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85465fa1",
   "metadata": {},
   "source": [
    "### 2.1 Winsorise extreme interaction counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0091ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9‑th percentile = 1,170,168; clipped values now ≤ that.\n"
     ]
    }
   ],
   "source": [
    "q = df[\"metric_value\"].quantile(0.999)\n",
    "df[\"metric_w\"] = np.where(df[\"metric_value\"] > q, q, df[\"metric_value\"])\n",
    "print(f\"99.9‑th percentile = {q:,.0f}; clipped values now ≤ that.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cd22b",
   "metadata": {},
   "source": [
    "## 3 | Pseudo‑shares & state encoding (1…2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b8ea1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shares(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    daily_tot = df.groupby(\"date\")[\"metric_w\"].transform(\"sum\")\n",
    "    df[\"share\"] = df[\"metric_w\"] / daily_tot\n",
    "    return df\n",
    "\n",
    "df = add_shares(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c9141",
   "metadata": {},
   "source": [
    "### 3.1 Pivot to wide \"state\" format (rank per page per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e8bedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping date → endpoint → rank, then fill missing as 2001\n",
    "rank_piv = (\n",
    "    df.pivot(index=\"date\", columns=\"endpoint_id\", values=\"rank\")\n",
    "      .fillna(2001).astype(np.int16)\n",
    ")\n",
    "\n",
    "share_piv = (\n",
    "    df.pivot(index=\"date\", columns=\"endpoint_id\", values=\"share\")\n",
    "      .fillna(0.0).astype(np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69baad",
   "metadata": {},
   "source": [
    "## 4 | 7‑day Transition Matrices  $P^{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c809e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition(df_rank: pd.DataFrame, weekday: str) -> np.ndarray:\n",
    "    \"\"\"Return (2001×2001) 7-day transition matrix for a given weekday.\"\"\"\n",
    "    rows   = df_rank.loc[df_rank.index.day_name() == weekday]\n",
    "    t0     = rows.values[:-1]          # today\n",
    "    t7     = rows.shift(-7).values[:-1]  # 7 days later (contains NaN)\n",
    "    \n",
    "    # keep only rows that have no NaNs *after* the shift\n",
    "    valid  = ~np.isnan(t7).any(axis=1)\n",
    "    t0, t7 = t0[valid], t7[valid]\n",
    "    \n",
    "    # ⚠️ cast back to int after NaNs are gone\n",
    "    t0 = t0.astype(np.int16)\n",
    "    t7 = t7.astype(np.int16)\n",
    "\n",
    "    counts = np.zeros((2001, 2001), dtype=np.int64)\n",
    "    for r0, r7 in zip(t0, t7):\n",
    "        np.add.at(counts, (r0 - 1, r7 - 1), 1)\n",
    "\n",
    "    P = counts / counts.sum(axis=1, keepdims=True)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac297d",
   "metadata": {},
   "source": [
    "## 5 | Parameter Estimation (σ, κ, η, β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42f664f5-329f-460e-a367-275b39b61621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated σ² = -7.566e-03  →  σ = 0.00000\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5 | Parameter Estimation  (σ, κ, η, β̂)\n",
    "###############################################################################\n",
    "# Helper: first‑day rank vector ( used for all group‑by operations )\n",
    "first_day_rank = rank_piv.iloc[0]                # Series: page → rank (1…2000)\n",
    "\n",
    "###############################################################################\n",
    "# 5.1  Variance‑of‑Δlog‑share funnel   →   σ\n",
    "###############################################################################\n",
    "log_sh_daily = share_piv.replace(0, np.nan).apply(np.log)\n",
    "rank_var     = log_sh_daily.diff().var()               # var of Δlog share, per page\n",
    "\n",
    "var_df = (\n",
    "    rank_var.groupby(first_day_rank)                   # group by initial rank\n",
    "            .mean()\n",
    "            .to_frame(\"var\")\n",
    "            .reset_index()\n",
    ")\n",
    "var_df.columns = [\"rank\", \"var\"]\n",
    "var_df[\"log_r\"] = np.log(var_df[\"rank\"])\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Clean the data: keep rows with finite, positive variance\n",
    "mask     = np.isfinite(var_df[\"var\"]) & (var_df[\"var\"] > 0)\n",
    "clean_df = var_df.loc[mask].copy()\n",
    "\n",
    "if len(clean_df) < 10:\n",
    "    raise RuntimeError(\"Too few valid rank rows to estimate σ²\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Weighted least‑squares:   var = σ² · log_r  +  intercept\n",
    "X  = clean_df[\"log_r\"].values\n",
    "y  = clean_df[\"var\"].values\n",
    "w  = np.sqrt(y)                       # weights = √variance   (optional)\n",
    "\n",
    "A  = np.vstack([X, np.ones_like(X)]).T\n",
    "Aw = A * w[:, None]\n",
    "yw = y * w\n",
    "\n",
    "try:\n",
    "    (sigma2, intercept), *_ = np.linalg.lstsq(Aw, yw, rcond=None)\n",
    "except np.linalg.LinAlgError:\n",
    "    # fall back to unweighted fit\n",
    "    print(\"⚠️  Weighted LS ill‑conditioned; falling back to un‑weighted fit.\")\n",
    "    (sigma2, intercept), *_ = np.linalg.lstsq(A, y, rcond=None)\n",
    "\n",
    "sigma2 = float(sigma2)\n",
    "sigma  = math.sqrt(max(sigma2, 1e-12))\n",
    "\n",
    "print(f\"Estimated σ² = {sigma2:.3e}  →  σ = {sigma:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad413f34-15d3-43bd-9c1e-d94852723bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated η = nan, κ = nan\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5.2  Drift line   →   κ, η\n",
    "# ---------------------------------------------------------------------------\n",
    "mean_dlog = log_sh_daily.diff().mean()           # mean Δlog share per page\n",
    "\n",
    "drift_df = (\n",
    "    mean_dlog.groupby(first_day_rank)\n",
    "             .mean()\n",
    "             .to_frame(\"dlog\")\n",
    "             .reset_index()\n",
    ")\n",
    "drift_df.columns = [\"rank\", \"dlog\"]\n",
    "\n",
    "# Simple linear fit  dlog = κ − η·rank   (note: slope is negative)\n",
    "slope, intercept = np.polyfit(drift_df[\"rank\"], drift_df[\"dlog\"], 1)\n",
    "eta   = -slope                                   # η > 0 by definition\n",
    "kappa = intercept\n",
    "print(f\"Estimated η = {eta:.4e}, κ = {kappa:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93ed7f67-7c9d-4765-a0a0-1142f260a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Could not write variance_time.csv: [Errno 1] Operation not permitted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band</th>\n",
       "      <th>beta</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-50</td>\n",
       "      <td>0.067563</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201-2000</td>\n",
       "      <td>0.036487</td>\n",
       "      <td>0.993178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51-200</td>\n",
       "      <td>0.045706</td>\n",
       "      <td>0.841369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       band      beta        R2\n",
       "0      1-50  0.067563  0.927602\n",
       "1  201-2000  0.036487  0.993178\n",
       "2    51-200  0.045706  0.841369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5.3  Variance–time exponent β (diagnostic only)\n",
    "# ---------------------------------------------------------------------------\n",
    "horizons   = np.array([1, 2, 4, 8, 28])          # days\n",
    "band_edges = [(1, 50), (51, 200), (201, 2000)]\n",
    "\n",
    "records = []\n",
    "for lo, hi in band_edges:\n",
    "    band_mask = first_day_rank.between(lo, hi)\n",
    "    for h in horizons:\n",
    "        v      = log_sh_daily.diff(h).iloc[h:]   # Δlog over horizon h\n",
    "        var_h  = v.loc[:, band_mask].var().mean()\n",
    "        records.append({\"band\": f\"{lo}-{hi}\", \"h\": h, \"var\": var_h})\n",
    "\n",
    "beta_df = pd.DataFrame(records)\n",
    "\n",
    "def var_power(t, A, beta):\n",
    "    return A * t ** beta\n",
    "\n",
    "beta_estimates = []\n",
    "for band, grp in beta_df.groupby(\"band\"):\n",
    "    g = grp[np.isfinite(grp[\"var\"])]             # drop NaN / inf rows\n",
    "    if len(g) < 3:\n",
    "        print(f\"⚠️  Skipping band {band}: <3 finite points\")\n",
    "        continue\n",
    "    popt, _      = curve_fit(var_power, g[\"h\"], g[\"var\"], p0=[1e-8, 1.0])\n",
    "    A_hat, b_hat = popt\n",
    "    ss_res       = ((g[\"var\"] - var_power(g[\"h\"], *popt))**2).sum()\n",
    "    ss_tot       = ((g[\"var\"] - g[\"var\"].mean())**2).sum()\n",
    "    R2           = 1 - ss_res / ss_tot\n",
    "    beta_estimates.append({\"band\": band, \"beta\": b_hat, \"R2\": R2})\n",
    "\n",
    "beta_tbl = pd.DataFrame(beta_estimates)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ensure the results directory exists (and is a directory, not a file)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    beta_tbl.to_csv(TABLE_DIR / \"variance_time.csv\", index=False)\n",
    "except PermissionError as e:\n",
    "    print(f\"⚠️  Could not write variance_time.csv: {e}\")\n",
    "except IsADirectoryError as e:\n",
    "    print(f\"⚠️  Path collision (expected a file): {e}\")\n",
    "\n",
    "display(beta_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214dd62",
   "metadata": {},
   "source": [
    "## 6 | Euler–Maruyama Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "309c4eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation finished; shape = (100, 365, 2001)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 6 | Euler–Maruyama Simulation  (with rank-padding safety)\n",
    "###############################################################################\n",
    "def simulate_paths(n_paths: int = 10_000, n_days: int = 365) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate rank-based diffusion paths.\n",
    "    Returns an array shaped (n_paths, n_days, 2001) holding daily shares.\n",
    "    Index 0…1999 = ranks 1…2000, index 2000 = absorbing '>2000' bucket.\n",
    "    \"\"\"\n",
    "    # --- initial weight vector w0 (length 2000) -----------------------------\n",
    "    # Use mean share per rank on the first observed date; pad missing ranks.\n",
    "    mean_by_rank = (\n",
    "        df.loc[df[\"date\"] == df[\"date\"].min()]\n",
    "          .groupby(\"rank\")[\"share\"]\n",
    "          .mean()\n",
    "          .reindex(range(1, 2001))            # ranks 1…2000\n",
    "          .fillna(0.0)\n",
    "          .to_numpy(np.float32)\n",
    "    )\n",
    "    # Any still-zero entries get a tiny epsilon so log() is safe.\n",
    "    eps = 1e-12\n",
    "    w0  = np.where(mean_by_rank == 0, eps, mean_by_rank)\n",
    "    # Normalise so Σ w0 = 1 and leave zero mass for the '>2000' bucket.\n",
    "    w0  = w0 / w0.sum()\n",
    "\n",
    "    # --- allocate output ----------------------------------------------------\n",
    "    out = np.zeros((n_paths, n_days, 2001), dtype=np.float32)\n",
    "    out[:, 0, :2000] = w0                      # broadcast to all paths\n",
    "    # last column (index 2000) stays 0 at t=0\n",
    "\n",
    "    # --- pre-compute drift and volatility vectors ---------------------------\n",
    "    ranks = np.arange(1, 2001, dtype=np.float32)\n",
    "    drift = kappa - eta * ranks\n",
    "    vol   = sigma * np.sqrt(np.log(ranks))\n",
    "\n",
    "    for p in range(n_paths):\n",
    "        w = w0.copy()\n",
    "        for t in range(1, n_days):\n",
    "            # geometric Brownian step\n",
    "            dB = np.random.normal(scale=np.sqrt(1.0), size=2000)\n",
    "            w  = w * np.exp(drift + vol * dB)\n",
    "            # force minimum epsilon, then renormalise\n",
    "            w  = np.maximum(w, eps)\n",
    "            w /= w.sum()\n",
    "\n",
    "            out[p, t, :2000] = w\n",
    "            # mass that drifted below rank-2000 could be added to index 2000\n",
    "            # but here we keep it 0 since we don't simulate dropouts yet.\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Simulate a small batch for diagnostic plots\n",
    "_sim = simulate_paths(n_paths=100, n_days=365)\n",
    "print(\"Simulation finished; shape =\", _sim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a272c",
   "metadata": {},
   "source": [
    "## 7 | Diagnostics & Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54e0938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/k06hqyxx2tsfxm64z7yqyc000000gn/T/ipykernel_3440/2824224873.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Funnel-of-churn  ·  empirical daily  (rank t-1  →  Δlog share t)\n",
    "###############################################################################\n",
    "eps      = 1e-12                                  # tiny additive constant\n",
    "log_sh   = np.log(share_piv + eps)                # no NaNs, no –inf\n",
    "dlog     = log_sh.diff().iloc[1:]                 # Δlog share (t − t−1)\n",
    "rank_t1  = rank_piv.iloc[:-1]                     # rank at t−1\n",
    "rank_t1.index = dlog.index                        # align on dates\n",
    "\n",
    "df_fun   = (\n",
    "    pd.concat({\"dlog\": dlog.stack(), \"rank\": rank_t1.stack()}, axis=1)\n",
    "      .astype({\"rank\": \"int32\"})\n",
    ")\n",
    "\n",
    "# Optional compact scatter: random 1 % sample + slight x-jitter\n",
    "sample   = df_fun.sample(frac=0.01, random_state=42)\n",
    "jitter   = np.random.uniform(-0.03, 0.03, size=len(sample))\n",
    "x_vals   = np.log(sample[\"rank\"].values) + jitter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(\n",
    "    x_vals,\n",
    "    sample[\"dlog\"],\n",
    "    s=1, alpha=0.08, linewidths=0, marker=\".\"\n",
    ")\n",
    "ax.set_xlabel(\"log(rank at day t−1)\")\n",
    "ax.set_ylabel(\"Δ log share (day t − day t−1)\")\n",
    "ax.set_title(\"Funnel of churn – empirical (daily)\")\n",
    "ax.set_ylim(sample[\"dlog\"].quantile(0.001), sample[\"dlog\"].quantile(0.999))\n",
    "ax.set_xlim(-0.1, np.log(2000) + 0.2)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"funnel_empirical.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1bcfe",
   "metadata": {},
   "source": [
    "*Repeat diagnostic plots and survival‑prob tables as per prompt…*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ad4fe",
   "metadata": {},
   "source": [
    "## 8 | Conclusions / Next Steps\n",
    "\n",
    "* Summarise parameter values and goodness‑of‑fit metrics.\n",
    "* Note whether a jump component seems necessary (left‑tail diagnostics).\n",
    "* Outline work to port model to Reddit and to extend list length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6533e",
   "metadata": {},
   "source": [
    "### Provenance\n",
    "Data source: CrowdTangle export of top‑2 000 U.S. Facebook pages, 2023‑01‑01 → 2024‑01‑31.  \n",
    "Notebook autogenerated by OpenAI o3 assistant (conversation ID …).  \n",
    "Run date: {{ cookiecutter.date }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
