---
title: 'Rank Diffusion on FB Pages: Stochastic Portfolio Theory-style Model'
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
params:
  data_path: "./fb_top2000_ranked_daily.parquet"
  horizons:
  - 1
  - 7
  - 28
  ranks_keep:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  - 256
  - 512
  - 1024
  ranks_overlay:
  - 2
  - 8
  - 32
  - 128
  - 512
  top_n_anchor: 1000
  anchor_days:
  - 1
  - 8
  - 15
  - 22
  m_head: 10
  index_top_n: null
  min_n_per_rank: 10
  quantiles:
  - 0.1
  - 0.25
  - 0.5
  - 0.75
  - 0.9
  k_daily:
  - 100
  - 200
  - 300
  - 400
  - 500
  - 600
  - 700
  - 800
  - 900
  weekly_quantiles:
  - 0.1
  - 0.5
  - 0.9
  seed: 1823
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4.6, dpi = 150)
set.seed(params$seed)
```

# Introduction

Goal: build a rank diffusion model using daily interaciton data from FB pages data. We hope to: 
* Use stochastic portfolio theory (SPT) ideas on relative shares and ranks to study drift and diffusion by rank. 
* Address leakage bias from the top-N list. 
* Compare models by daily and weekly aggregation.

Core aim is to have working code that we can test on the smaller top-N dataset, and the port to the full ~400K top post dataset from Instagram and Facebook and the full 

## Basics of a rank diffusion model.

Let $A_i(t)$ be interactions for page $i$ on day $t$. Define share (also sometimes calls weight from SPT) as  $w_i(t)=A_i(t)/\sum_j A_j(t)$. 

Rank pages by descending $w_i(t)$ to get $r_i(t)\in{1,2,\dots}$. A rank diffusion model posits that $\Delta\log w_i$ over a horizon $H$ has rank-specific drift $b_{r}$ and variance (\sigma^2_{r}). In discrete time:
$$
g_i(t;H)=\log A_i(t+H)-\log A_i(t),\quad
\Delta\log w_i(t;H)=\log w_i(t+H)-\log w_i(t)
$$

Conditioning on rank at $t$ or $t-H$ gives us rank-based drift and variance diagnostics. In SPT, the capital-distribution curve (CDC) summarizes the time-average of $w(r)$ across ranks. In our case this is really the attention/interaction distribution curve, but we will use the CDC terminology for consistency. 

## SPT lessons for platform data.

SPT treats $w_i$ as a dynamical system. State at time $t$ is just the vector of shares:  weights must be >= 0 and sum of weights == 1. 

B/c model estimates relative composition, rank-conditioning is essential. In stationary regimes the CDC should be stable and expected rank drift mean-reverting at the top. Diffusion may be near-constant (Atlas-like) or increase with rank (e.g., $\sigma^2_r\propto \log r)$. 

## Open questions 

**Does variance of $\Delta\log w$ increase with (\log r) within the top ranks or is it near-constant?**  We want to build a model that nests the simplest constant variance model (Atlas model).  

> Looks like it is near-constant! Very close to Atlas model.

**What is the shape and magnitude of rank drift at the top?** Initial analysis suggests mean $\Delta\log w_i(t;H)$ for all of the top 1k ranks is below zero -- but actually lower for topmost ranks.  Does this hold up? What about weekly data or changes as a 7-day moving-average  

**Impact of leakage**: many pages have no posting or near-zero interactions on some days. We have to figure out how to handle this. Possibilities in order: 

* aggregation (start w/ week)
* changes as moving averages
* condition just on active pages 
* jump model.

> weekly aggregration works pretty well!   
> More work to be done on how many lags are necessary b/f correlation goes to zero


# Libraries, helpers, and reproducibility

Need to check for library availability, required columns, deterministic ranking; must give a reproducible seed.

```{r}
suppressPackageStartupMessages({
  library(tidyverse)
  library(arrow)
  library(lubridate)
  library(scales)
  library(broom)
  library(here)
  library(purrr)
})

theme_set(theme_minimal())

req_cols <- c("date","endpoint_id","metric_value")

read_any <- function(path) {
  ext <- tolower(tools::file_ext(path))
  if (ext %in% c("parquet","feather")) return(arrow::read_parquet(path))
  if (ext %in% c("csv","tsv")) return(readr::read_delim(path, delim = ifelse(ext=="csv", ",", "\t"), show_col_types = FALSE))
  stop("Unsupported file extension: ", ext)
}

safe_rank1 <- function(x, decreasing = TRUE) base::rank(if (decreasing) -x else x, ties.method = "first")
log_brks <- function(rmax) { b <- 2^(0:floor(log2(rmax))); b[b <= rmax] }
```

# Load, clean, and daily/weekly shares and ranks

This section: load daily interactions, keep strictly positive rows, compute daily shares and ranks.

Checks:
* Required columns present?
* do dates parse? 
* are pages per day stable?
* no duplicated (date, endpoint_id) rows, sane share distribution.

```{r load}
raw <- read_any(params$data_path) %>% as_tibble()
stopifnot(all(req_cols %in% names(raw)))

daily <- raw %>%
  mutate(
    date = as.Date(date),
    endpoint_id = as.character(endpoint_id)
  ) %>%
  filter(metric_value > 0) %>%
  group_by(date) %>%
  mutate(
    total_metric = sum(metric_value, na.rm = TRUE),
    share        = metric_value / total_metric
  ) %>%
  ungroup()

if (!"rank" %in% names(daily)) {
  daily <- daily %>%
    group_by(date) %>%
    mutate(rank = as.integer(safe_rank1(share, decreasing = TRUE))) %>%
    ungroup()
}

# Drop dates with sparse rank coverage so downstream growth pairs stay stable
filter_low_rank_dates <- function(daily_tbl, min_ranks = 1850) {
  counts <- daily_tbl %>% count(date, name = "n")
  sparse_dates <- counts %>% filter(n < min_ranks)

  list(
    daily = daily_tbl %>% anti_join(sparse_dates, by = "date"),
    removed_dates = sparse_dates
  )
}

filter_result <- filter_low_rank_dates(daily, min_ranks = 1850)
daily <- filter_result$daily
removed_low_rank_dates <- filter_result$removed_dates

if (nrow(removed_low_rank_dates) > 0) {
  message(
    sprintf(
      "Filtered %d dates with < %d ranks (min = %d).",
      nrow(removed_low_rank_dates),
      1850,
      min(removed_low_rank_dates$n)
    )
  )
}

# Diagnostics
dup_ct <- daily %>% count(date, endpoint_id) %>% filter(n>1) %>% nrow()
day_counts <- daily %>% count(date)
tibble(
  rows = nrow(daily),
  days = n_distinct(daily$date),
  unique_pages = n_distinct(daily$endpoint_id),
  min_pages_per_day = min(day_counts$n),
  median_pages_per_day = median(day_counts$n),
  max_pages_per_day = max(day_counts$n),
  duplicated_pairs = dup_ct
) %>% knitr::kable(caption = "Dataset diagnostics")

ggplot(daily, aes(x = share)) +
  geom_histogram(bins = 80) +
  scale_x_log10(labels = percent_format(accuracy = 0.01)) +
  labs(title = "Share distribution", x = "Share (log)", y = "Count")

```

```{r weekly_build}
# --- Weekly aggregation (moved earlier) -------------------------------------
weekly <- daily %>%
  mutate(week_start = floor_date(date, unit = "week", week_start = 1)) %>%  # Monday
  group_by(week_start, endpoint_id) %>%
  summarise(weekly_metric = sum(metric_value, na.rm = TRUE), .groups = "drop") %>%
  group_by(week_start) %>%
  mutate(
    weekly_total = sum(weekly_metric, na.rm = TRUE),
    weekly_share = weekly_metric / weekly_total,
    rank_w = as.integer(rank(-weekly_share, ties.method = "first"))
  ) %>%
  ungroup()

# --- Weekly pairs: observed (t, t+H) with metric and share changes ----------
build_weekly_pairs_observed <- function(weekly, H = 1, max_rank = Inf) {
  w_all   <- sort(unique(weekly$week_start))
  valid_t <- w_all[(w_all + weeks(H)) %in% w_all]

  base <- weekly %>%
    filter(week_start %in% valid_t, rank_w <= max_rank) %>%
    transmute(week_start, endpoint_id,
              rank_t = rank_w,
              a_t    = weekly_metric,
              w_t    = weekly_share)

  later <- weekly %>%
    transmute(week_start = week_start - weeks(H), endpoint_id,
              a_tpH   = weekly_metric,
              w_tpH   = weekly_share,
              rank_tpH = rank_w)

  base %>%
    inner_join(later, by = c("week_start","endpoint_id")) %>%
    mutate(horizon = H,
           g       = log(a_tpH) - log(a_t),
           dlog    = log(w_tpH) - log(w_t))
}

# --- Weekly leakage (presence-based or top-N) -------------------------------
compute_weekly_leakage <- function(weekly, horizons = c(1,2,3,4), ranks_keep = NULL, top_n = NULL) {
  if (is.null(ranks_keep)) ranks_keep <- sort(unique(weekly$rank_w))

  base_all <- weekly %>% select(week_start, rank_w, endpoint_id) %>% filter(rank_w %in% ranks_keep)
  if (!is.null(top_n)) base_all <- base_all %>% filter(rank_w <= top_n)
  w_all <- sort(unique(weekly$week_start))

  out <- lapply(horizons, function(H) {
    valid_t <- w_all[(w_all + weeks(H)) %in% w_all]
    baseH   <- base_all %>% filter(week_start %in% valid_t) %>% rename(rank = rank_w)
    later   <- weekly %>% transmute(week_start = week_start - weeks(H), endpoint_id, rank_later = rank_w)
    if (!is.null(top_n)) later <- later %>% filter(rank_later <= top_n)

    joined <- baseH %>% left_join(later, by = c("week_start","endpoint_id")) %>%
      mutate(leaked = is.na(rank_later), horizon = H)

    joined %>% group_by(week_start, rank, horizon) %>%
      summarise(leakage = mean(leaked), .groups = "drop")
  }) %>% bind_rows()

  summary <- out %>% group_by(rank, horizon) %>%
    summarise(leakage_rate = mean(leakage), n_weeks = n(), .groups = "drop")

  list(daily = out, summary = summary)
}
```

So definitely a few issues.  

Context here: CrowdTangle was **insanely** buggy and unreliable in its last few months. There was one period we could not get reliable data on.  

But the histogram of share distribution looks mostly exactly as it should, except for some low-end outliers. 

Let's look at a plot of low-end outliers by date and rank.  

```{r outliers_low_share}
# Diagnostic: low-end outliers (share < 0.03%) by date
low_thr <- 0.0003  # 0.03%
outliers_low <- daily %>% filter(share < low_thr)

ggplot(outliers_low, aes(x = date, y = rank)) +
  geom_point(alpha = 0.35, size = 0.6) +
  # To display rank 1 at top, uncomment:
  # scale_y_reverse() +
  labs(
    title = "Low-end outliers: share < 0.03%",
    subtitle = paste("n =", scales::comma(nrow(outliers_low))),
    x = "Date", y = "Rank"
  )

```

Let's plot that collection timeline. 

# Checking collection timeline / pages per day

This is how things look filtering out days with fewer than 1850 ranks: 


```{r colletion-timeline}

# Coverage timeline: pages per day
pages_per_day <- daily %>% count(date, name = "pages")

ggplot(pages_per_day, aes(x = date, y = pages)) +
  geom_line() +
  geom_hline(yintercept = median(pages_per_day$pages), linetype = "dashed") +
  labs(
    title = "Coverage timeline: pages per day",
    subtitle = paste0(
      "min = ", min(pages_per_day$pages),
      ", median = ", median(pages_per_day$pages),
      ", max = ", max(pages_per_day$pages)
    ),
    x = "Date", y = "Pages"
  )
```

**TO DO**--further diagnostics and cleaning growth pairs from problematic dates.  


# Interaction distribution curve / capital-distribution curve (CDC)

This section: estimate the CDC and head exponent.A log-log linear head suggests Pareto-like scaling; we are expecting tail to be curvilinear and decreasing. 

OLS fit is not really best practice here b/c it overweights tail, but good enough for this initial analysis.

Checks: 
* Head fit linear in log-log scale?  
* Exponent consistent in different segments of the curve?
* Any weirdness at rank 1?

```{r cdc}

# Mean share by rank
mean_share <- daily %>%
  group_by(rank) %>%
  summarise(w_bar = mean(share, na.rm = TRUE), .groups = "drop")

# Head and midrange fits on log-log scale
fit_head <- lm(log(w_bar) ~ log(rank), data = filter(mean_share, rank >= 1, rank <= 10))
fit_mid  <- lm(log(w_bar) ~ log(rank), data = filter(mean_share, rank >= 11, rank <= 100))
fit_low <- lm(log(w_bar) ~ log(rank), data = filter(mean_share, rank >= 101, rank <= 1000))

# Summaries (alpha = -slope)
summarise_fit <- function(fit, label){
  s <- tidy(fit, conf.int = TRUE) %>% filter(term == "log(rank)")
  tibble(
    segment  = label,
    alpha    = -s$estimate,
    alpha_lo = -s$conf.high,   # note flip because alpha = -beta
    alpha_hi = -s$conf.low,
    R2       = summary(fit)$r.squared
  )
}
cdc_fits <- bind_rows(
  summarise_fit(fit_head, "ranks 1–10"),
  summarise_fit(fit_mid,  "ranks 11–100"),
  summarise_fit(fit_low,  "ranks 101–1000")
)
knitr::kable(cdc_fits, digits = 4, caption = "CDC power-law fits on log–log scale")

# Predicted lines on original (share) scale so they match the log–log axes
pred_head <- tibble(rank = 1:10) %>%
  mutate(w_pred = exp(coef(fit_head)[[1]]) * rank^(coef(fit_head)[[2]]),
         segment = "fit 1–10")
pred_mid  <- tibble(rank = 11:100) %>%
  mutate(w_pred = exp(coef(fit_mid)[[1]])  * rank^(coef(fit_mid)[[2]]),
         segment = "fit 11–100")
pred_low  <- tibble(rank = 101:1000) %>%
  mutate(w_pred = exp(coef(fit_low)[[1]])  * rank^(coef(fit_low)[[2]]),
         segment = "fit 101–1000")
pred_all  <- bind_rows(pred_head, pred_mid, pred_low)

# Plot: points + both fitted lines; axes on log–log
ggplot(mean_share, aes(rank, w_bar)) +
  geom_point(alpha = 0.6, size = 1.2) +
  geom_line(data = pred_all, aes(y = w_pred, colour = segment), linewidth = 0.9) +
  scale_x_log10() +
  scale_y_log10(labels = percent_format(accuracy = 0.01)) +
  labs(
    title   = "Capital-distribution curve (time-averaged)",
    subtitle= "Overlaid power-law fits for ranks 1–10, 11-100 and 101–1,000 (log–log OLS)",
    x = "Rank (log)", y = "Mean share (log)", colour = "Fit window"
  )


```

Let's try the same thing with median value by rank instead of mean value.  

```{r cdc_median_segments}
# Median CDC segmented fits: 1–10, 11–100, 101–1000
median_share <- daily %>%
  group_by(rank) %>%
  summarise(w_med = median(share, na.rm = TRUE), .groups = "drop")

# Fit helpers
fit_seg <- function(rmin, rmax) lm(log(w_med) ~ log(rank), data = median_share %>% filter(rank >= rmin, rank <= rmax))
seg1 <- fit_seg(1, 10)
seg2 <- fit_seg(11, 100)
seg3 <- fit_seg(101, 1000)

seg_tbl <- function(fit, lbl) {
  s <- broom::tidy(fit, conf.int = TRUE) %>% dplyr::filter(term == "log(rank)")
  tibble(
    segment  = lbl,
    alpha    = -s$estimate,
    alpha_lo = -s$conf.high,
    alpha_hi = -s$conf.low,
    R2       = summary(fit)$r.squared,
    n        = fit$df.residual + length(fit$coefficients)
  )
}

cdc_med_fits <- bind_rows(
  seg_tbl(seg1, "1–10"),
  seg_tbl(seg2, "11–100"),
  seg_tbl(seg3, "101–1000")
)
knitr::kable(cdc_med_fits, digits = 4, caption = "Median CDC: segmented power-law fits (log–log)")

# Overlay predicted lines (original scale; log axes)
pred1 <- tibble(rank = 1:10) %>%
  mutate(w_pred = exp(coef(seg1)[[1]]) * rank^(coef(seg1)[[2]]),
         segment = "fit 1–10")
pred2 <- tibble(rank = 11:100) %>%
  mutate(w_pred = exp(coef(seg2)[[1]]) * rank^(coef(seg2)[[2]]),
         segment = "fit 11–100")
pred3 <- tibble(rank = 101:1000) %>%
  mutate(w_pred = exp(coef(seg3)[[1]]) * rank^(coef(seg3)[[2]]),
         segment = "fit 101–1000")
pred_med <- bind_rows(pred1, pred2, pred3)

ggplot(median_share, aes(rank, w_med)) +
  geom_point(alpha = 0.6, size = 1.1) +
  geom_line(data = pred_med, aes(y = w_pred, colour = segment), linewidth = 0.9) +
  scale_x_log10() +
  scale_y_log10(labels = scales::percent_format(accuracy = 0.01)) +
  labs(
    title = "Median capital-distribution curve with segmented fits",
    x = "Rank (log)", y = "Median share (log)", colour = "Fit window"
  )
```

Reactions:  Big difference in slope fitting at different parts of the head!  

Median plot does reduce weirdness with top rank but otherwise very similar.  

# Build pairs and construct growth rates

This section: construct observed growth pairs for metrics and shares at horizon (H).

Separate metric growth $g=\log A(t+H)-\log A(t)$ from share changes $\Delta\log w$. Calculate variance and drift based on starting consistency.

Checks: 
* Consistent results with different horizons? 
* How many zero/missing pairs?  Adequate counts per rank?

```{r growth rates}
# Daily metric growth pairs: g = log A(t+H) − log A(t)
build_metric_pairs <- function(daily, H = 1, ranks_keep = NULL) {
  base <- daily %>% transmute(date, endpoint_id, rank_t = as.integer(rank), a_t = metric_value)
  later <- daily %>% transmute(date = date - days(H), endpoint_id, a_tpH = metric_value, rank_tpH = as.integer(rank))
  out <- base %>% inner_join(later, by = c("date","endpoint_id")) %>% filter(a_t > 0, a_tpH > 0) %>%
    mutate(horizon = H, g = log(a_tpH) - log(a_t))
  if (!is.null(ranks_keep)) out <- out %>% filter(rank_t %in% ranks_keep)
  out
}

# Daily share change pairs for SPT primitives
build_share_pairs <- function(daily, H = 1) {
  daily %>%
    arrange(endpoint_id, date) %>%
    group_by(endpoint_id) %>%
    mutate(
      log_share  = log(share),
      dlog       = log_share - lag(log_share, H),
      rank_prev  = lag(rank, H),
      rank_curr  = rank,
      delta_days = as.integer(date - lag(date, H))
    ) %>%
    ungroup() %>%
    filter(!is.na(dlog), delta_days == H)
}


# Observed (t, t+H) metric growth pairs across horizons
pair_counts <- bind_rows(lapply(params$horizons, function(H) {
  build_metric_pairs(daily, H = H) %>%
    count(horizon, rank_t, name = "n")
}))

# Table (subset to ranks_keep for readability)
pair_counts_tbl <- pair_counts %>%
  filter(rank_t %in% params$ranks_keep) %>%
  tidyr::pivot_wider(names_from = horizon, values_from = n, names_prefix = "H=", values_fill = 0) %>%
  arrange(rank_t)

knitr::kable(pair_counts_tbl, caption = "Observed pair counts by start rank and horizon (metric growth)")

weekly_horizons <- c(1, 2, 3, 4)  # horizons in weeks for the weekly pairs

  weekly_pair_counts <- bind_rows(lapply(weekly_horizons, function(H) {
    build_weekly_pairs_observed(weekly, H = H) %>%
      count(horizon, rank_t, name = "n")
  })) %>%
    mutate(horizon = factor(horizon, levels = weekly_horizons))

  weekly_pair_counts_tbl <- weekly_pair_counts %>%
    filter(rank_t %in% params$ranks_keep) %>%
    mutate(horizon_label = paste0("W=", horizon)) %>%
    select(-horizon) %>%
    pivot_wider(names_from = horizon_label, values_from = n, values_fill = 0) %>%
    arrange(rank_t)

  knitr::kable(
    weekly_pair_counts_tbl,
    caption = "Observed weekly pair counts by start rank and horizon (metric growth)"
  )


```

```{r pair_coverage_heatmap}
# Portion of non-missing pairs by rank bin and horizon (1,7,28)
rank_bins <- list(
  "1"         = c(1,1),
  "2–3"       = c(2,3),
  "4–7"       = c(4,7),
  "8–15"      = c(8,15),
  "16–31"     = c(16,31),
  "32–63"     = c(32,63),
  "64–127"    = c(64,127),
  "128–255"   = c(128,255),
  "256–511"   = c(256,511),
  "512–1024"  = c(512,1024)
)

compute_pair_portion_by_bin <- function(daily, horizons = c(1,7,28)) {
  bin_map <- tibble(rank = sort(unique(daily$rank))) %>%
    mutate(bin = case_when(
      rank == 1 ~ "1",
      rank >= 2   & rank <= 3   ~ "2–3",
      rank >= 4   & rank <= 7   ~ "4–7",
      rank >= 8   & rank <= 15  ~ "8–15",
      rank >= 16  & rank <= 31  ~ "16–31",
      rank >= 32  & rank <= 63  ~ "32–63",
      rank >= 64  & rank <= 127 ~ "64–127",
      rank >= 128 & rank <= 255 ~ "128–255",
      rank >= 256 & rank <= 511 ~ "256–511",
      rank >= 512 & rank <= 1024 ~ "512–1024",
      TRUE ~ NA_character_
    ))

  daily_b <- daily %>% inner_join(bin_map, by = "rank") %>% filter(!is.na(bin))
  all_dates <- sort(unique(daily_b$date))

  bind_rows(lapply(horizons, function(H) {
    valid_t <- all_dates[(all_dates + days(H)) %in% all_dates]

    base  <- daily_b %>% filter(date %in% valid_t) %>% select(date, endpoint_id, bin)
    later <- daily_b %>% transmute(date = date - days(H), endpoint_id)

    cand <- base %>% count(date, bin, name = "n_cand")
    obs  <- base %>% inner_join(later, by = c("date","endpoint_id")) %>% count(date, bin, name = "n_obs")

    out <- cand %>%
      full_join(obs, by = c("date","bin")) %>%
      mutate(n_cand = replace_na(n_cand, 0L),
             n_obs  = replace_na(n_obs,  0L)) %>%
      group_by(bin) %>%
      summarise(n_cand = sum(n_cand), n_obs = sum(n_obs), .groups = "drop") %>%
      mutate(horizon = H,
             portion = if_else(n_cand > 0, n_obs / n_cand, NA_real_))
    out
  }))
}

pair_portion <- compute_pair_portion_by_bin(daily, horizons = c(1,7,28))
pair_portion$bin <- factor(pair_portion$bin, levels = names(rank_bins))

ggplot(pair_portion, aes(x = bin, y = factor(horizon), fill = portion)) +
  geom_tile(color = "white") +
  scale_fill_gradient(limits = c(0,1), low = "white", high = "steelblue",
                      labels = scales::percent, na.value = "grey90", name = "Portion") +
  labs(
    title = "Portion of non-missing pairs by rank bin and horizon",
    x = "Rank bin", y = "Horizon (days)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


# Variance funnel and drift (H = 1): Atlas vs log-rank

Look if $Var \Delta\log w$ scales with $\log(r)$ or is near-constant. Estimate drift by current rank.

Checks
* Compare WLS fits by AIC/BIC. 
* Drift should be negative at very top in a stable regime.
* Outliers should not dominate.

```{r var-drift}
H1_share <- build_share_pairs(daily, H = 1)

# Variance conditional on rank_prev
var_by_rank <- H1_share %>%
  group_by(rank_prev) %>%
  summarise(var_dlog = var(dlog, na.rm = TRUE), n = n(), .groups = "drop") %>%
  mutate(log_rank = log(rank_prev), weight = pmax(n - 1, 1))

fit_funnel_lin <- lm(var_dlog ~ log_rank, data = var_by_rank, weights = weight)
fit_funnel_cte <- lm(var_dlog ~ 1,         data = var_by_rank, weights = weight)

comp_var <- tibble(
  model = c("Var ~ log(rank_prev)","Var ~ constant"),
  AIC   = c(AIC(fit_funnel_lin), AIC(fit_funnel_cte)),
  BIC   = c(BIC(fit_funnel_lin), BIC(fit_funnel_cte)),
  slope = c(unname(coef(fit_funnel_lin)["log_rank"]), NA_real_),
  R2    = c(summary(fit_funnel_lin)$r.squared, NA_real_)
)
knitr::kable(comp_var, digits = 4, caption = "Variance model comparison (WLS)")

ggplot(var_by_rank, aes(log_rank, var_dlog)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.8) +
  labs(title = "Funnel diagnostic (H=1; condition on rank at t−1)", subtitle = paste0("WLS slope = ", signif(coef(fit_funnel_lin)[["log_rank"]],3), ", R² = ", round(summary(fit_funnel_lin)$r.squared,3)), x = "log(rank_prev)", y = "Var[Δ log share]")


```
```{r weekly drift and funnel}
W1_share <- build_weekly_pairs_observed(weekly, H = 1) %>%
    mutate(rank_prev = rank_t)

  var_by_rank_weekly <- W1_share %>%
    group_by(rank_prev) %>%
    summarise(var_dlog = var(dlog, na.rm = TRUE), n = n(), .groups = "drop") %>%
    filter(is.finite(var_dlog)) %>%
    mutate(log_rank = log(rank_prev), weight = pmax(n - 1, 1))

  fit_weekly_lin <- lm(var_dlog ~ log_rank, data = var_by_rank_weekly, weights = weight)
  fit_weekly_cte <- lm(var_dlog ~ 1,         data = var_by_rank_weekly, weights = weight)

  comp_var_weekly <- tibble(
    model = c("Var ~ log(rank_prev)", "Var ~ constant"),
    AIC   = c(AIC(fit_weekly_lin), AIC(fit_weekly_cte)),
    BIC   = c(BIC(fit_weekly_lin), BIC(fit_weekly_cte)),
    slope = c(unname(coef(fit_weekly_lin)["log_rank"]), NA_real_),
    R2    = c(summary(fit_weekly_lin)$r.squared, NA_real_)
  )

  knitr::kable(comp_var_weekly, digits = 4, caption = "Weekly variance model comparison (WLS)")
```


```{r weekly var v rank plot}
W1_share <- build_weekly_pairs_observed(weekly, H = 1) %>%
    mutate(rank_prev = rank_t)

  var_by_rank_weekly <- W1_share %>%
    group_by(rank_prev) %>%
    summarise(var_dlog = var(dlog, na.rm = TRUE), n = n(), .groups = "drop") %>%
    filter(is.finite(var_dlog), var_dlog > 0) %>%
    mutate(log_rank = log(rank_prev), weight = pmax(n - 1, 1))

  var_by_rank_weekly_top1k <- var_by_rank_weekly %>% filter(rank_prev <= 1000)
  var_by_rank_weekly_2_1000 <- var_by_rank_weekly %>% filter(rank_prev >= 2, rank_prev <= 1000)

  fit_weekly_lin <- lm(var_dlog ~ log_rank, data = var_by_rank_weekly, weights = weight)
  fit_weekly_lin_top1k <- lm(var_dlog ~ log_rank, data = var_by_rank_weekly_top1k, weights = weight)
  fit_weekly_lin_2_1000 <- if (nrow(var_by_rank_weekly_2_1000) >= 2) {
    lm(var_dlog ~ log_rank, data = var_by_rank_weekly_2_1000, weights = weight)
  } else {
    NULL
  }

  fitted_segment <- if (!is.null(fit_weekly_lin_2_1000)) {
    var_by_rank_weekly_2_1000 %>%
      mutate(fitted = predict(fit_weekly_lin_2_1000, newdata = var_by_rank_weekly_2_1000)) %>%
      arrange(rank_prev)
  } else {
    tibble()
  }

  subtitle_parts <- c(
    "All ranks WLS slope =",
    signif(coef(fit_weekly_lin)[["log_rank"]], 3),
    "· R² =",
    round(summary(fit_weekly_lin)$r.squared, 3),
    "| ranks ≤ 1,000 slope =",
    signif(coef(fit_weekly_lin_top1k)[["log_rank"]], 3),
    "· R² =",
    round(summary(fit_weekly_lin_top1k)$r.squared, 3)
  )

  if (!is.null(fit_weekly_lin_2_1000)) {
    subtitle_parts <- c(
      subtitle_parts,
      "| ranks 2–1,000 slope =",
      signif(coef(fit_weekly_lin_2_1000)[["log_rank"]], 3),
      "· R² =",
      round(summary(fit_weekly_lin_2_1000)$r.squared, 3)
    )
  }

  ggplot(var_by_rank_weekly, aes(x = rank_prev, y = var_dlog, weight = weight)) +
    geom_point(alpha = 0.6) +
    geom_smooth(
      method = "lm", formula = y ~ log(x), se = FALSE,
      linewidth = 0.8, colour = "#1b9e77"
    ) +
    geom_smooth(
      data = var_by_rank_weekly_top1k,
      method = "lm", formula = y ~ log(x), se = FALSE,
      linewidth = 0.8, colour = "#d95f02"
    ) +
    geom_line(
      data = fitted_segment,
      aes(x = rank_prev, y = fitted),
      colour = "#7570b3",
      linewidth = 0.9,
      inherit.aes = FALSE
    ) +
    scale_x_log10() +
    scale_y_log10() +
    labs(
      title = "Weekly funnel diagnostic (H = 1 week; condition on rank at week t−1)",
      subtitle = paste(subtitle_parts, collapse = " "),
      x = "Rank at week t (log scale)",
      y = "Var[Δ log weekly share] (log scale)"
    )

```

The weekly data is almost perfectly flat -- strong evidence for the Atlas (constant variance model)




# Leakage off index by rank

This section will measure leakage at ranks over different time horizons (H).

Core issue is that inconsistent posting and intereaction/attention sparsity causes missing observations. Leakage is doubly problematic because it introduces selection bias/censoring in growth distributions.

Checks:
* Leakage rates should be between 0 & 1 (sanity check).
* Leakage should grow monotonically by time horizon (esp. after 7 days; may be day-of-week effects.
* Expected leakage should grow monotonically with rank. 
* Pages near threshold should drop off the list slightly > 50% of the time. 


```{r leakage}
compute_index_leakage <- function(daily, horizons = c(1,7,28), top_n = NULL, ranks_keep = NULL) {
  stopifnot(all(c("date","endpoint_id","rank") %in% names(daily)))
  if (is.null(ranks_keep)) ranks_keep <- sort(unique(daily$rank))

  base_all <- daily %>% select(date, rank, endpoint_id) %>% filter(rank %in% ranks_keep)
  if (!is.null(top_n)) base_all <- base_all %>% filter(rank <= top_n)

  all_dates <- sort(unique(daily$date))

  out <- lapply(horizons, function(H) {
    valid_t <- all_dates[(all_dates + days(H)) %in% all_dates]
    baseH <- base_all %>% filter(date %in% valid_t)
    later <- daily %>% transmute(date = date - days(H), endpoint_id, rank_later = rank)
    if (!is.null(top_n)) later <- later %>% filter(rank_later <= top_n)

    joined <- baseH %>%
      left_join(later, by = c("date","endpoint_id")) %>%
      mutate(leaked = is.na(rank_later), horizon = H)

    joined %>%
      group_by(date, rank, horizon) %>%
      summarise(leakage = mean(leaked), .groups = "drop")
  }) %>% bind_rows()

  summary <- out %>% group_by(rank, horizon) %>%
    summarise(leakage_rate = mean(leakage), n_days = n(), .groups = "drop")

  list(daily = out, summary = summary)
}

L <- compute_index_leakage(
  daily,
  horizons = params$horizons,
  top_n = params$index_top_n
)

ggplot(L$summary %>% arrange(horizon, rank), aes(x = rank, y = leakage_rate, colour = factor(horizon), group = factor(horizon))) +
  geom_line(linewidth = 0.9) +
  scale_x_log10(labels = number_format(accuracy = 1)) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(title = "Leakage off index by rank", subtitle = if (is.null(params$index_top_n)) "Presence-based index" else paste0("Top-", params$index_top_n, " index"), x = "Rank (log scale)", y = "Leakage rate", colour = "H (days)")

L_weekly <- compute_weekly_leakage(
  weekly,
  horizons = c(1, 2, 3, 4),
  top_n = params$index_top_n
)

L_weekly_summary <- L_weekly$summary %>%
  filter(rank <= 1024) %>%
  arrange(horizon, rank)

ggplot(L_weekly_summary, aes(x = rank, y = leakage_rate, colour = factor(horizon), group = factor(horizon))) +
  geom_line(linewidth = 0.9) +
  scale_x_log10(labels = number_format(accuracy = 1)) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(title = "Weekly leakage off index by rank", subtitle = if (is.null(params$index_top_n)) "Presence-based weekly index" else paste0("Top-", params$index_top_n, " index"), x = "Rank at week t (log scale)", y = "Leakage rate", colour = "H (weeks)")


# For share-change pairs, quantify how much we drop due to non-consecutive dates at lag H
compute_continuity_loss <- function(daily, H_set) {
  bind_rows(lapply(H_set, function(H) {
    tmp <- daily %>%
      arrange(endpoint_id, date) %>%
      group_by(endpoint_id) %>%
      mutate(prev_date = lag(date, H),
             delta_days = as.integer(date - prev_date)) %>%
      summarise(
        total_pairs = sum(!is.na(prev_date)),
        kept_pairs  = sum(delta_days == H, na.rm = TRUE),
        .groups = "drop"
      ) %>%
      summarise(
        total_pairs = sum(total_pairs),
        kept_pairs  = sum(kept_pairs),
        .groups = "drop"
      ) %>%
      mutate(
        horizon       = H,
        kept_share    = kept_pairs / pmax(total_pairs, 1),
        filtered_share= 1 - kept_share
      ) %>%
      select(horizon, total_pairs, kept_pairs, kept_share, filtered_share)
  }))
}

loss_tbl <- compute_continuity_loss(daily, params$horizons)
knitr::kable(loss_tbl, digits = 4,
             caption = "Continuity of lag-H share pairs. Filtered_share = dropped due to delta_days ≠ H")

# Quick readable line for H=1 if present
if (any(loss_tbl$horizon == 1)) {
  fs <- loss_tbl$filtered_share[loss_tbl$horizon == 1][1]
  cat("Filtered share at H=1:", scales::percent(fs, accuracy = 0.1), "\n")
}

```

# Growth distributions by rank

Show rank-conditioned distributions of $g=\log A(t+H)-\log A(t)$ for daily horizons (1, 7, 28 days) alongside weekly horizons (1, 2, 4 weeks).

Daily curves use stayers only, while weekly curves aggregate the weekly metric panel. Lee bounds add worst-case monotone selection bounds for weekly quantiles using leakage rates.

Checks. Adequate observations within each rank/horizon window; leakage adjustments finite; identified sets shrink as leakage falls.

```{r leakage-adjusted-growth}
# Lee quantile bounds for vector x with leakage rate l
lee_bounds_one <- function(x, taus = c(0.1,0.5,0.9), l) {
  x <- sort(x[is.finite(x)])
  if (!length(x)) return(tibble(tau = taus, q_obs = NA_real_, q_lb = NA_real_, q_ub = NA_real_))
  qfun <- function(p) unname(quantile(x, probs = p, names = FALSE, type = 7))
  Gamma <- 1 - l
  tau_lb <- pmax(0, (taus - l) / Gamma)
  tau_ub <- pmin(1,  taus / Gamma)
  tibble(
    tau  = taus,
    q_obs = sapply(taus, qfun),
    q_lb  = sapply(tau_lb, qfun),
    q_ub  = sapply(tau_ub, qfun)
  )
}

# Build daily metric growth pairs for all horizons and selected ranks
G_all <- bind_rows(lapply(params$horizons, function(H) build_metric_pairs(daily, H = H, ranks_keep = params$ranks_keep))) %>% filter(is.finite(g))

plot_density_overlay <- function(df, ranks_overlay, horizons, weight_col = NULL, title_suffix = "") {
  df2 <- df %>% filter(rank_t %in% ranks_overlay, horizon %in% horizons)
  xr <- range(df2$g, finite = TRUE)
  purrr::map(horizons, function(H) {
    dH <- df2 %>% filter(horizon == H)
    if (!nrow(dH)) return(ggplot() + theme_void() + ggtitle(paste("No data for H =", H)))
    bwH <- tryCatch(stats::bw.SJ(dH$g), error = function(e) stats::bw.nrd0(dH$g))
    ggplot(dH, aes(x = g, colour = factor(rank_t))) +
      { if (is.null(weight_col)) geom_density(bw = bwH, linewidth = 0.9)
        else geom_density(bw = bwH, linewidth = 0.9, aes(weight = .data[[weight_col]])) } +
      geom_vline(xintercept = 0, linetype = "dotted") +
      coord_cartesian(xlim = xr) +
      labs(title = paste0("Rank-conditioned growth densities ", title_suffix, " | H = ", H, "d"), subtitle = paste0("Overlaid ranks: ", paste(ranks_overlay, collapse = ", ")), x = "log A(t+H) − log A(t)", y = "Density", colour = "Rank at t")
  })
}

# Unweighted (stayers only)
dens_unw <- plot_density_overlay(G_all, params$ranks_overlay, params$horizons, weight_col = NULL, title_suffix = "(unweighted)")
weekly_horizons <- c(1, 2, 4)
G_weekly_all <- bind_rows(lapply(weekly_horizons, function(H) build_weekly_pairs_observed(weekly, H = H))) %>%
  filter(is.finite(g))
dens_weekly <- plot_density_overlay(G_weekly_all, params$ranks_overlay, weekly_horizons, weight_col = NULL, title_suffix = "(weekly)")

purrr::walk(dens_unw, print)
purrr::walk(dens_weekly, print)

compute_bounds_by_rank <- function(G, Lsum, taus = c(0.1,0.5,0.9), min_n = 10) {
  G %>% group_by(horizon, rank_t) %>%
    summarise(n = n(), g_list = list(g), .groups = "drop") %>%
    filter(n >= min_n) %>%
    left_join(Lsum %>% transmute(rank, horizon, l = leakage_rate), by = c("rank_t" = "rank", "horizon" = "horizon")) %>%
    mutate(l = replace_na(l, 0)) %>%
    rowwise() %>% mutate(bounds = list(lee_bounds_one(unlist(g_list), taus = taus, l = l))) %>%
    unnest(bounds) %>% ungroup()
}

B_weekly <- compute_bounds_by_rank(G_weekly_all, L_weekly$summary, taus = params$quantiles, min_n = params$min_n_per_rank) %>%
  filter(rank_t <= 1024)

ggplot(B_weekly, aes(x = rank_t)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey55") +
  geom_ribbon(aes(ymin = q_lb, ymax = q_ub, fill = factor(tau)), alpha = 0.18) +
  geom_line(aes(y = q_obs, colour = factor(tau)), linewidth = 0.8) +
  scale_x_log10(labels = number_format(accuracy = 1)) +
  facet_wrap(~ horizon, scales = "free_y") +
  labs(title = "Weekly leakage-robust quantile bands by rank (Lee bounds)", subtitle = "Shaded = identified set; line = observed stayer quantile", x = "Rank at week t (log scale)", y = "Quantile of log A_w(t+H) − log A_w(t)", fill = "Quantile", colour = "Quantile")
```

# Anchor-day churn funnel

Objective. Compare interactions at anchors vs (+7) days for the top pages on those anchors.

Rationale. Visualizes short-run churn among leaders and the degree of regression toward the mean.

Checks. Symmetry around the 45° line for stable regimes. Excess mass far below the line signals rapid decay or inactivity.

```{r churn funnel}
plot_churn_funnel <- function(daily, top_n = 1000, anchor_days = c(1,8,15,22), horizon = 7, facet = FALSE) {
  anchors <- daily %>%
    mutate(dom = day(date)) %>%
    filter(dom %in% anchor_days) %>%
    distinct(date, dom) %>%
    arrange(date)

  d0 <- daily %>%
    inner_join(anchors, by = "date") %>%
    group_by(date) %>%
    slice_max(order_by = share, n = top_n, with_ties = FALSE) %>%
    ungroup() %>%
    transmute(date, endpoint_id, x = metric_value, dom)

  dH <- daily %>% transmute(date = date - days(horizon), endpoint_id, y = metric_value)

  pairs <- d0 %>% inner_join(dH, by = c("date","endpoint_id")) %>% filter(x > 0, y > 0)

  p <- ggplot(pairs, aes(x = x, y = y, colour = factor(dom))) +
    geom_point(alpha = 0.25, size = 1) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_log10(labels = label_number(scale_cut = cut_short_scale())) +
    scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +
    labs(title = "Churn funnel: anchor day vs +7d", subtitle = paste0("Top ", top_n, " by share on days ", paste(anchor_days, collapse = "/"), " | points = ", scales::comma(nrow(pairs))), x = "Interactions on anchor day (log)", y = "Interactions +7d (log)", colour = "DOM")
  if (facet) p <- p + facet_wrap(~ factor(dom, levels = anchor_days), nrow = 1)
  p
}

plot_churn_funnel(daily, top_n = params$top_n_anchor, anchor_days = params$anchor_days, horizon = 7, facet = FALSE)
```



# Rank-wise k-th largest daily growth curves

Objective. For each rank, plot the (k)-th highest observed (g) for (k\in{100,\dots,900}) at (H=1,7).

Rationale. Captures tail dynamics by rank without assuming parametric forms.

Checks. Points omitted when fewer than (k) pairs exist. Compare (H=1) vs (H=7) with matched y-limits.

```{r kth-highest growth}
build_growth_pairs_observed <- function(daily, H, max_rank = 900) {
  all_dates <- sort(unique(daily$date))
  valid_t   <- all_dates[(all_dates + days(H)) %in% all_dates]

  base_t <- daily %>%
    filter(date %in% valid_t, rank <= max_rank) %>%
    transmute(date, endpoint_id, rank_t = as.integer(rank), a_t = metric_value)

  later <- daily %>% transmute(date = date - days(H), endpoint_id, a_tpH = metric_value)

  base_t %>%
    inner_join(later, by = c("date","endpoint_id")) %>%
    mutate(g = log(a_tpH) - log(a_t))
}

compute_rank_order_curves <- function(daily, H, ks = seq(100, 900, by = 100), max_rank = 900) {
  pairs <- build_growth_pairs_observed(daily, H = H, max_rank = max_rank)
  pairs %>%
    group_by(rank_t) %>%
    summarise(g_sorted = list(sort(g, decreasing = TRUE)), n = n(), .groups = "drop") %>%
    tidyr::crossing(k = ks) %>%
    mutate(g_k = purrr::map2_dbl(g_sorted, k, ~ if (length(.x) >= .y) .x[.y] else NA_real_),
           horizon = H) %>%
    select(rank_t, k, g_k, n, horizon)
}

plot_rank_order_curves <- function(curves, ks = seq(100, 900, by = 100), y_limits = NULL, title_prefix = "k-th highest log growth") {
  df <- curves %>% filter(k %in% ks)
  if (is.null(y_limits)) y_limits <- range(df$g_k, na.rm = TRUE)
  ggplot(df, aes(x = rank_t, y = g_k, colour = factor(k), group = factor(k))) +
    geom_hline(yintercept = 0, linetype = "dashed", colour = "grey50") +
    geom_line(na.rm = TRUE, linewidth = 0.9) +
    geom_point(na.rm = TRUE, size = 0.8) +
    scale_x_log10(breaks = c(log_brks(900), 900)) +
    coord_cartesian(ylim = y_limits) +
    labs(title = sprintf("%s — H = %sd", title_prefix, unique(df$horizon)), subtitle = "Lines are k ∈ {100,200,…,900}; points omitted when fewer than k observed pairs", x = "Rank (log scale)", y = "k-th highest log A(t+H) − log A(t)", colour = "k")
}

ks_use <- params$k_daily
curves_H1 <- compute_rank_order_curves(daily, H = 1, ks = ks_use, max_rank = 900)
curves_H7 <- compute_rank_order_curves(daily, H = 7, ks = ks_use, max_rank = 900)
y_lim_global <- range(c(curves_H1$g_k, curves_H7$g_k), na.rm = TRUE)

plot_rank_order_curves(curves_H1, ks = ks_use, y_limits = y_lim_global)
plot_rank_order_curves(curves_H7, ks = ks_use, y_limits = y_lim_global)
```

# Weekly aggregation and week→week growth

Objective. Aggregate to Monday–Sunday weeks, recompute shares and ranks, then study week→week growth.

Rationale. Weekly aggregation reduces daily sparsity and mitigates leakage. It should clarify the core model.

Checks. Weekly steps exist for most weeks. Distributions smoother. Tail curves consistent with daily but less noisy.

```{r weekly}
# Aggregate to weeks and recompute ranks
weekly <- daily %>%
  mutate(week_start = floor_date(date, unit = "week", week_start = 1)) %>%  # Monday
  group_by(week_start, endpoint_id) %>%
  summarise(weekly_metric = sum(metric_value, na.rm = TRUE), .groups = "drop") %>%
  group_by(week_start) %>%
  mutate(
    weekly_total = sum(weekly_metric, na.rm = TRUE),
    weekly_share = weekly_metric / weekly_total,
    rank_w = as.integer(safe_rank1(weekly_share, decreasing = TRUE))
  ) %>%
  ungroup()

# Observed week→week pairs
build_weekly_pairs_observed <- function(weekly, H = 1, max_rank = Inf) {
  w_all  <- sort(unique(weekly$week_start))
  valid_t <- w_all[(w_all + weeks(H)) %in% w_all]

  base <- weekly %>%
    filter(week_start %in% valid_t, rank_w <= max_rank) %>%
    transmute(week_start, endpoint_id, rank_t = rank_w, a_t = weekly_metric)

  later <- weekly %>% transmute(week_start = week_start - weeks(H), endpoint_id, a_tpH = weekly_metric)

  base %>% inner_join(later, by = c("week_start","endpoint_id")) %>%
    mutate(g = log(a_tpH) - log(a_t))
}

pairs_w1 <- build_weekly_pairs_observed(weekly, H = 1)

# Distributions for ranks {1,4,16,64,128}
ranks_dist <- c(1,4,16,64,128)
dist_df <- pairs_w1 %>% filter(rank_t %in% ranks_dist)
bw_all <- tryCatch(stats::bw.SJ(dist_df$g), error = function(e) stats::bw.nrd0(dist_df$g))

ggplot(dist_df, aes(x = g, colour = factor(rank_t))) +
  geom_density(bw = bw_all, linewidth = 1) +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "grey50") +
  labs(title = "Weekly log growth distributions (week→week)", subtitle = "Overlay for start ranks 1, 4, 16, 64, 128", x = "log A_w(t+1) − log A_w(t)", y = "Density", colour = "Start rank")

# Coverage
n_weeks_total <- n_distinct(weekly$week_start)
valid_weeks   <- sort(unique(weekly$week_start))
n_steps_pairs <- sum((valid_weeks + weeks(1)) %in% valid_weeks)
tibble(weekly_periods = n_weeks_total, week_to_week_steps = n_steps_pairs) %>% knitr::kable(caption = "Weekly coverage")

# Rank-wise k-th highest weekly growth (k ∈ {10,25,40})
compute_weekly_rank_order_curves <- function(weekly, ks = c(10,25,40), max_rank = NULL) {
  pairs <- build_weekly_pairs_observed(weekly, H = 1, max_rank = ifelse(is.null(max_rank), Inf, max_rank))
  pairs %>%
    group_by(rank_t) %>%
    summarise(g_sorted = list(sort(g, decreasing = TRUE)), n = n(), .groups = "drop") %>%
    tidyr::crossing(k = ks) %>%
    mutate(g_k = purrr::map2_dbl(g_sorted, k, ~ if (length(.x) >= .y) .x[.y] else NA_real_)) %>%
    filter(!is.na(g_k)) %>%
    arrange(rank_t, k)
}

curves_weekly <- compute_weekly_rank_order_curves(weekly, ks = c(10,25,40))

ggplot(curves_weekly, aes(x = rank_t, y = g_k, colour = factor(k), group = factor(k))) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "grey50") +
  geom_line(linewidth = 1) + geom_point(size = 0.8) +
  scale_x_log10(breaks = log_brks(max(curves_weekly$rank_t, na.rm = TRUE))) +
  labs(title = "Weekly growth: k-th highest by start rank (H = 1 week)", subtitle = "k ∈ {10, 25, 40}; points omitted where fewer than k observations", x = "Start rank (log scale)", y = "k-th highest log A_w(t+1) − log A_w(t)", colour = "k")

```

`# Summary and next steps

```{r CDC}

#### ---- CDC estimation from WEEKLY rank growth (Atlas-style) ----

suppressPackageStartupMessages({
  library(dplyr); library(ggplot2); library(scales); library(tidyr); library(purrr)
})

# ---------- Helpers ----------
.standardize_weekly <- function(df, cols=list(
  rank="rank",
  mean="mean_log_growth_week",
  median="median_log_growth_week",
  var="var_log_growth_week",
  share="activity_share_week"    # optional
)) {
  stopifnot(is.data.frame(df))
  required <- c("rank","mean","median","var")
  miss <- setdiff(required, names(cols))
  if(length(miss)) stop("Missing mapping(s) in `cols`: ", paste(miss, collapse=", "))
  # rename safely if present
  present <- intersect(names(cols), names(df))
  if(length(present)==0) stop("No expected columns found. Map your column names via `cols=`.")
  out <- df %>% rename(any_of(cols))
  # check required exist
  need <- c("rank","mean","median","var")
  if(!all(need %in% names(out))) stop("After renaming, required columns not found: ",
                                      paste(setdiff(need, names(out)), collapse=", "))
  # coerce
  out %>% mutate(rank=as.integer(rank)) %>% arrange(rank)
}

.trimmed_mean <- function(x, trim=0.1) mean(x, trim=trim, na.rm=TRUE)

# Isotonic smoothing in rank (monotone increasing g_k)
.iso_smooth <- function(k, y) {
  o <- order(k); k <- k[o]; y <- y[o]
  iso <- stats::isoreg(k, y)
  # predict at integer k via piecewise-constant fit
  yf <- approx(x = iso$x, y = iso$yf, xout = k, method="constant", rule=2, f=0)$y
  yf[order(o)]  # return in original order
}

# Fit tail g_k ≈ -a (k+b)^(-alpha) on the shoulder/tail window
.fit_tail_power <- function(k, g, k0=NULL, b=1) {
  df <- tibble(k=k, g=g) %>% filter(k>= (k0 %||% floor(0.6*max(k))), g < -1e-8)
  if(nrow(df) < 10) return(list(ok=FALSE))
  fit <- try(lm(log(-g) ~ I(log(k + b)), data=df), silent=TRUE)
  if(inherits(fit,"try-error")) return(list(ok=FALSE))
  alpha <- -unname(coef(fit)[2]); a <- exp(unname(coef(fit)[1]))
  r2 <- summary(fit)$r.squared
  list(ok=TRUE, a=a, b=b, alpha=alpha, r2=r2, k0=min(df$k))
}

# Extend tail g_k using fitted power law; fallback to gentle exponential if needed
.extend_g_tail <- function(g_head, K, Kmax, tail_fit) {
  k <- seq_len(Kmax)
  g <- numeric(Kmax); g[seq_len(K)] <- g_head
  if(isTRUE(tail_fit$ok) && tail_fit$alpha > 0) {
    a <- tail_fit$a; b <- tail_fit$b; alpha <- tail_fit$alpha
    g[(K+1):Kmax] <- -a * ( ( (K+1):Kmax ) + b )^(-alpha)
  } else {
    # fallback: exponential decay to 0 from last observed g_K
    gK <- g_head[K]; lam <- 0.005
    g[(K+1):Kmax] <- gK * exp(-lam * ( (K+1):Kmax - K ))
  }
  g
}

# Construct CDC from g_k and sigma^2 under Atlas: r_k = -2 G(k) / sigma^2,  Δm_k = -1/r_k
.build_cdc <- function(g, sigma2) {
  Kmax <- length(g)
  G <- cumsum(g)
  # enforce negative partial sums (stationarity proxy). If violated, shift g by a small constant.
  if(any(G[1:(Kmax-1)] >= 0)) {
    shift <- (max(G[1:(Kmax-1)]) + 1e-6) / seq_len(Kmax)[which.max(G[1:(Kmax-1)])]
    g <- g - shift
    G <- cumsum(g)
  }
  r <- -2*G/sigma2
  # guard against nonpositive r in head
  r[r <= 1e-9] <- NA_real_
  dm <- -1/r
  m <- numeric(Kmax); m[1] <- 0
  for(k in 1:(Kmax-1)) m[k+1] <- m[k] + dm[k]
  list(m=m, r=r, G=G, dm=dm, g=g)
}

# Normalize mu_k ∝ exp(m_k) and compute shares
.normalize_mu <- function(m, tol=1e-12) {
  mu_raw <- exp(m - max(m, na.rm=TRUE))  # for numerical stability
  mu_raw[is.na(mu_raw)] <- 0
  Z <- sum(mu_raw)
  mu <- mu_raw / Z
  list(mu=mu, Z=Z)
}

# ---------- MAIN PIPELINE ----------
estimate_cdc_atlas_weekly <- function(df_weekly,
                                      K=1000,
                                      cols=list(
                                        rank="rank",
                                        mean="mean_log_growth_week",
                                        median="median_log_growth_week",
                                        var="var_log_growth_week",
                                        share="activity_share_week"  # optional
                                      ),
                                      plot_prefix="weekly") {

  df <- .standardize_weekly(df_weekly, cols) %>%
    filter(rank >= 1) %>%
    arrange(rank) %>%
    mutate(.keep="unused")

  # Use only top K
  topK <- df %>% filter(rank <= K)

  # Constant variance estimate (pooled, robust)
  sigma2_trim <- .trimmed_mean(topK$var, trim=0.1)
  sigma2_med  <- median(topK$var, na.rm=TRUE)
  # Trend check of var vs log-rank
  var_trend <- tryCatch({
    m <- lm(var ~ log(rank), data=topK)
    c(slope = unname(coef(m)[2]), p = summary(m)$coefficients[2,4])
  }, error=function(e) c(slope=NA_real_, p=NA_real_))

  # Monotone drift schedule g_k from rank means (remove a rank-invariant intercept)
  gamma_hat <- median(topK$mean, na.rm=TRUE)   # market-growth intercept proxy
  y_center  <- topK$mean - gamma_hat
  g_iso     <- .iso_smooth(k=topK$rank, y=y_center)

  # Tail fit: g_k ≈ -a (k+b)^(-alpha)
  tail_fit <- .fit_tail_power(k=topK$rank, g=g_iso, k0=floor(0.6*K), b=1)

  # Choose sigma^2 (prefer flatness if trend p>0.05 else trimmed mean anyway)
  sigma2 <- as.numeric(sigma2_trim)

  # Build CDC up to large Kmax by extrapolating g_k
  Kmax <- 200000  # high cap; will normalize so mass in far tail is negligible
  g_all <- .extend_g_tail(g_head=g_iso, K=K, Kmax=Kmax, tail_fit=tail_fit)

  cdc <- .build_cdc(g=g_all, sigma2=sigma2)
  mu_norm <- .normalize_mu(cdc$m)
  mu <- mu_norm$mu

  # Determine effective support where mass becomes negligible
  cutoff <- which(cumsum(mu) > (1 - 1e-6))[1]
  if(is.na(cutoff)) cutoff <- Kmax
  keep <- seq_len(cutoff)

  # Shares
  share_topK      <- sum(mu[pmin(K, length(mu)) %>% seq_len()])
  share_topK      <- sum(mu[seq_len(min(K, length(mu)))])
  total_activity  <- 1.0     # normalized
  topK_activity   <- share_topK

  # ---------- OUTPUTS ----------
  # 1) Smoothed growth vs rank (log-x)
  p_growth <- topK %>%
    mutate(mean=topK$mean, median=topK$median, g_iso=g_iso) %>%
    ggplot(aes(x=rank)) +
    geom_point(aes(y=mean), alpha=0.35, size=0.8) +
    geom_line(aes(y=g_iso + gamma_hat), linewidth=0.7) +
    geom_smooth(aes(y=mean), method="loess", formula=y~log10(x), span=0.25, se=FALSE, linewidth=0.6, linetype="longdash") +
    scale_x_log10(labels=label_number_si()) +
    labs(title="Weekly log-growth by rank (top 1000)",
         subtitle="Points: means; solid: isotonic (monotone) fit + intercept; dashed: LOESS (light smoothing). X is log10(rank).",
         x="Rank (log10 scale)", y="Weekly mean log-growth") +
    theme_minimal(base_size=11)

  # 2) Intermediate metrics
  metrics <- tibble::tibble(
    metric = c("sigma2_pooled_trimmed", "sigma2_pooled_median",
               "var_vs_logrank_slope", "var_vs_logrank_p",
               "gamma_hat_intercept", "tail_power_alpha", "tail_power_a", "tail_fit_R2"),
    value  = c(sigma2_trim, sigma2_med,
               var_trend["slope"], var_trend["p"],
               gamma_hat,
               if(isTRUE(tail_fit$ok)) tail_fit$alpha else NA_real_,
               if(isTRUE(tail_fit$ok)) tail_fit$a else NA_real_,
               if(isTRUE(tail_fit$ok)) tail_fit$r2 else NA_real_)
  )

  # 3) Functional form for g_k (reported)
  g_form <- if(isTRUE(tail_fit$ok) && tail_fit$alpha>0) {
    sprintf("g_k ≈ monotone head (isotonic on k ≤ %d), then tail g_k = -a (k+%d)^{-alpha} with a=%.4g, alpha=%.3f.",
            K, tail_fit$b, tail_fit$a, tail_fit$alpha)
  } else {
    "g_k ≈ monotone head (isotonic on k ≤ K), with an exponentially decaying fallback tail."
  }

  # 4) CDC plot (log–log). Real data in thick black if activity_share_week is present.
  cdc_df <- tibble(
    rank = keep,
    mu_est = mu[keep],
    segment = ifelse(rank <= K, "model<=K", "model>1000")
  )

  # empirical shares if available
  have_emp <- "share" %in% names(topK) && all(is.finite(topK$share))
  if(have_emp) {
    emp <- topK %>% select(rank, share) %>% mutate(share = pmax(share, .Machine$double.eps))
  } else {
    emp <- NULL
  }

  p_cdc <- ggplot() +
    { if(have_emp) geom_line(data=emp, aes(x=rank, y=share),
                             linewidth=1.0, color="black") } +
    geom_line(data=cdc_df %>% filter(rank>1000),
              aes(x=rank, y=mu_est), linewidth=0.7, linetype="dashed", color="grey70") +
    geom_line(data=cdc_df %>% filter(rank<=1000),
              aes(x=rank, y=mu_est), linewidth=0.6, color="grey40") +
    scale_x_log10(labels=label_number_si()) +
    scale_y_log10(labels=label_number_si()) +
    labs(title="Estimated capital distribution curve (CDC)",
         subtitle=paste0("Empirical (black) where available; model beyond 1000 shown dashed light gray. Total normalized to 1. ",
                         "Top-1000 share = ", scales::percent(topK_activity, accuracy=0.01)),
         x="Rank (log10)", y="Activity share μ_(k) (log10)") +
    theme_minimal(base_size=11)

  # 5) Sanity checks
  #   (i) partial sums negative
  neg_partial <- all(cdc$G[1:(min(K, length(cdc$G))-1)] < 0, na.rm=TRUE)
  #   (ii) r_k positive
  pos_r <- all(cdc$r[1:(min(K, length(cdc$r))-1)] > 0, na.rm=TRUE)
  #   (iii) tail mass small by cutoff
  tail_mass <- 1 - sum(mu[seq_len(min(K, length(mu)))])
  checks <- tibble::tibble(
    check = c("Partial sums G(k)<0 for k≤K-1", "Gap params r_k>0 for k≤K-1",
              "Mass outside top K (model)"),
    result = c(neg_partial, pos_r, tail_mass)
  )

  list(
    plots=list(growth=p_growth, cdc=p_cdc),
    metrics=metrics,
    checks=checks,
    g_form=g_form,
    model=list(
      K=K, sigma2=sigma2, gamma_hat=gamma_hat,
      g_head=g_iso, tail_fit=tail_fit,
      m=cdc$m, r=cdc$r, G=cdc$G,
      mu=mu, cutoff=cutoff,
      topK_share=topK_activity, total_activity=total_activity
    )
  )
}

# ---------- RUN (set `df_weekly` and column mapping if needed) ----------
# If your object/columns have different names, pass `cols=list(rank=..., mean=..., median=..., var=..., share=...)`.
res <- estimate_cdc_atlas_weekly(
  df_weekly = weekly_rank_stats,   # <-- replace if your object is named differently
  K = 1000
)

# ---------- DISPLAY ----------
print(res$plots$growth)
print(res$plots$cdc)

knitr::kable(res$metrics, digits=4, caption="Intermediate metrics used in CDC estimation.")
knitr::kable(res$checks, digits=4, caption="Sanity checks.")

cat("\nEstimated functional form for g_k:\n", res$g_form, "\n")
cat("\nEstimated top-1000 share of total activity (model, normalized): ",
    scales::percent(res$model$topK_share, accuracy=0.01), "\n")


```
⸻
