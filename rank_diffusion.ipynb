{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72137fd8",
   "metadata": {},
   "source": [
    "# Rank‑Based Diffusion | Facebook Pages (Top 2 000)\n",
    "*Proof‑of‑concept analysis notebook — generated 2025‑07‑24*\n",
    "\n",
    "This notebook implements the workflow defined in **Prompt v0.2**:\n",
    "\n",
    "1. Data QC and pseudo‑share construction  \n",
    "2. Weekday‑aware 7‑day transition matrices  \n",
    "3. Parameter estimation (σ, κ, η, β̂)  \n",
    "4. Euler–Maruyama simulation & diagnostics  \n",
    "5. Artefact export (`results/`)\n",
    "\n",
    "> **Re‑run instructions**  \n",
    "> • Requires Python 3.11 + `numpy`, `pandas`, `duckdb`, `pyarrow`, `matplotlib`, `scipy`.  \n",
    "> • Set the variable `PROJECT_ROOT` below to your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dd40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1 | Setup & imports                                                          #\n",
    "################################################################################\n",
    "import os, sys, json, math, itertools, gzip, datetime, pathlib, warnings\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 120, \"axes.spines.top\": False,\n",
    "                     \"axes.spines.right\": False})\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = pathlib.Path(\"./data/rank-diffusion\")               # ← change if needed\n",
    "DATA_PROMPT   = PROJECT_ROOT/.\"data/rank-diffusion/data/fb_top2000_ranked_daily.parquet\"\n",
    "DATA_UPLOAD   = pathlib.Path(\"/mnt/data/fb_top2000_ranked_daily.parquet\")\n",
    "\n",
    "RESULTS_DIR   = PROJECT_ROOT/\"results\"\n",
    "FIG_DIR       = RESULTS_DIR/\"figures\"\n",
    "TABLE_DIR     = RESULTS_DIR/\"tables\"\n",
    "MATRIX_DIR    = RESULTS_DIR/\"matrices\"\n",
    "\n",
    "for p in [FIG_DIR, TABLE_DIR, MATRIX_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db54dd",
   "metadata": {},
   "source": [
    "## 2 | Load daily panel & basic QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672722e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Parquet file not found in either location.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweekday\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday_name()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows spanning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ▸ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mload_panel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m DATA_PROMPT \u001b[38;5;28;01mif\u001b[39;00m DATA_PROMPT\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;28;01melse\u001b[39;00m DATA_UPLOAD\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquet file not found in either location.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m con \u001b[38;5;241m=\u001b[39m ddb\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    SELECT date, endpoint_id, metric_value, rank\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    FROM parquet_scan(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfetch_df()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Parquet file not found in either location."
     ]
    }
   ],
   "source": [
    "def load_panel() -> pd.DataFrame:\n",
    "    \"\"\"Read the Parquet file via DuckDB streaming; return a pandas DF.\"\"\"\n",
    "    path = DATA_PROMPT if DATA_PROMPT.exists() else DATA_UPLOAD\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\"Parquet file not found in either location.\")\n",
    "    con = ddb.connect()\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT date, endpoint_id, metric_value, rank\n",
    "        FROM parquet_scan('{path}')\n",
    "    \"\"\").fetch_df()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "    df[\"weekday\"] = df[\"date\"].dt.day_name()\n",
    "    return df\n",
    "\n",
    "df = load_panel()\n",
    "print(df.head())\n",
    "print(f\"Loaded {len(df):,} rows spanning {df['date'].min()} ▸ {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81201a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick missing‑date check\n",
    "all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"D\")\n",
    "missing   = sorted(set(all_dates.date) - set(df[\"date\"].dt.date))\n",
    "print(f\"Missing calendar dates: {len(missing)} (listed if ≤10)\")\")\n",
    "print(missing[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85465fa1",
   "metadata": {},
   "source": [
    "### 2.1 Winsorise extreme interaction counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0091ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = df[\"metric_value\"].quantile(0.999)\n",
    "df[\"metric_w\"] = np.where(df[\"metric_value\"] > q, q, df[\"metric_value\"])\n",
    "print(f\"99.9‑th percentile = {q:,.0f}; clipped values now ≤ that.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cd22b",
   "metadata": {},
   "source": [
    "## 3 | Pseudo‑shares & state encoding (1…2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ea1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shares(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    daily_tot = df.groupby(\"date\")[\"metric_w\"].transform(\"sum\")\n",
    "    df[\"share\"] = df[\"metric_w\"] / daily_tot\n",
    "    return df\n",
    "\n",
    "df = add_shares(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c9141",
   "metadata": {},
   "source": [
    "### 3.1 Pivot to wide \"state\" format (rank per page per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping date → endpoint → rank, then fill missing as 2001\n",
    "rank_piv = (\n",
    "    df.pivot(index=\"date\", columns=\"endpoint_id\", values=\"rank\")\n",
    "      .fillna(2001).astype(np.int16)\n",
    ")\n",
    "\n",
    "share_piv = (\n",
    "    df.pivot(index=\"date\", columns=\"endpoint_id\", values=\"share\")\n",
    "      .fillna(0.0).astype(np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69baad",
   "metadata": {},
   "source": [
    "## 4 | 7‑day Transition Matrices  $P^{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition(df_rank: pd.DataFrame, weekday: str) -> np.ndarray:\n",
    "    \"\"\"Return (2001×2001) transition probability matrix for given weekday.\"\"\"\n",
    "    rows = df_rank.loc[df_rank.index.day_name()==weekday]\n",
    "    t0   = rows.values[:-1]\n",
    "    t7   = rows.shift(-7).values[:-1]\n",
    "    valid = ~np.isnan(t7).any(axis=1)\n",
    "    t0, t7 = t0[valid], t7[valid]\n",
    "    counts = np.zeros((2001, 2001), dtype=np.int64)\n",
    "    for r0, r7 in zip(t0, t7):\n",
    "        np.add.at(counts, (r0-1, r7-1), 1)\n",
    "    P = counts / counts.sum(axis=1, keepdims=True)\n",
    "    return P\n",
    "\n",
    "weekday_list = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "for wd in weekday_list:\n",
    "    P = build_transition(rank_piv, wd)\n",
    "    np.savez_compressed(MATRIX_DIR/f\"P_{wd}.npz\", P=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac297d",
   "metadata": {},
   "source": [
    "## 5 | Parameter Estimation (σ, κ, η, β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Variance funnel → σ\n",
    "rank_var = (share_piv.apply(np.log).diff().var())\n",
    "var_df = rank_var.groupby(rank_piv.iloc[0]).mean().to_frame(\"var\")\n",
    "var_df[\"log_r\"] = np.log(var_df.index)\n",
    "sigma2, _ = np.polyfit(var_df[\"log_r\"], var_df[\"var\"], 1)\n",
    "sigma = math.sqrt(max(sigma2, 1e-12))\n",
    "print(f\"Estimated σ² = {sigma2:.3e}  ⇒  σ = {sigma:.4f}\")\n",
    "\n",
    "# 5.2 Drift → κ, η\n",
    "mean_dlog = share_piv.apply(np.log).diff().mean()\n",
    "drift_df  = mean_dlog.groupby(rank_piv.iloc[0]).mean().to_frame(\"dlog\")\n",
    "eta, kappa = np.polyfit(drift_df.index, drift_df[\"dlog\"], 1)\n",
    "eta   = -eta  # slope is negative\n",
    "print(f\"Estimated η = {eta:.4e}, κ = {kappa:.4e}\")\n",
    "\n",
    "# 5.3 Variance‑time exponent β (check)\n",
    "horizons = np.array([1,2,4,8,28])\n",
    "band_edges = [(1,50),(51,200),(201,2000)]\n",
    "records = []\n",
    "log_sh  = share_piv.apply(np.log)\n",
    "for lo, hi in band_edges:\n",
    "    for h in horizons:\n",
    "        v = log_sh.diff(h).iloc[h:]\n",
    "        band_mask = rank_piv.iloc[0].between(lo, hi)\n",
    "        records.append({\"band\":f\"{lo}-{hi}\", \"h\":h,\n",
    "                        \"var\": v.loc[:, band_mask].var().mean()})\n",
    "beta_df = pd.DataFrame(records)\n",
    "\n",
    "def var_power(t, A, beta):\n",
    "    return A * t**beta\n",
    "\n",
    "beta_estimates = []\n",
    "for band, grp in beta_df.groupby(\"band\"):\n",
    "    popt, pcov = curve_fit(var_power, grp[\"h\"], grp[\"var\"], p0=[1e-8,1.0])\n",
    "    A_hat, beta_hat = popt\n",
    "    ss_res = ((grp[\"var\"] - var_power(grp[\"h\"], *popt))**2).sum()\n",
    "    ss_tot = ((grp[\"var\"] - grp[\"var\"].mean())**2).sum()\n",
    "    R2 = 1 - ss_res/ss_tot\n",
    "    beta_estimates.append({\"band\":band,\"beta\":beta_hat,\"R2\":R2})\n",
    "beta_tbl = pd.DataFrame(beta_estimates)\n",
    "beta_tbl.to_csv(TABLE_DIR/\"variance_time.csv\", index=False)\n",
    "beta_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214dd62",
   "metadata": {},
   "source": [
    "## 6 | Euler–Maruyama Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_paths(n_paths:int=10000, n_days:int=365):\n",
    "    \"\"\"Return array shaped (n_paths, n_days, 2001) of simulated shares.\"\"\"\n",
    "    w0 = var_df.index.to_series().sort_index().index  # placeholder equal weights\n",
    "    w0 = var_df.index.map(lambda r: df.loc[df['rank']==r,'share'].mean()).values\n",
    "    w0 = np.where(w0==0, 1e-12, w0)\n",
    "    w0 = w0 / w0.sum()\n",
    "    out = np.empty((n_paths, n_days, 2001), dtype=np.float32)\n",
    "    out[:,:,2000] = 0.0  # absorbing prob mass; placeholder\n",
    "    for p in range(n_paths):\n",
    "        w = w0.copy()\n",
    "        out[p,0,:2000] = w\n",
    "        for t in range(1, n_days):\n",
    "            drift = (kappa - eta*np.arange(1,2001))\n",
    "            vol   = sigma*np.sqrt(np.log(np.arange(1,2001)))\n",
    "            dB    = np.random.normal(scale=math.sqrt(1.0), size=2000)\n",
    "            w = w * np.exp(drift + vol*dB)\n",
    "            w = np.maximum(w, 1e-12)\n",
    "            w = w / w.sum()\n",
    "            out[p,t,:2000] = w\n",
    "    return out\n",
    "\n",
    "# Simulations are heavy → run a small batch for diagnostic\n",
    "_sim = simulate_paths(n_paths=100, n_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a272c",
   "metadata": {},
   "source": [
    "## 7 | Diagnostics & Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: funnel plot (empirical vs. simulation) for one random sample\n",
    "emp_dlog = log_sh.diff().stack().dropna()\n",
    "emp_rank = rank_piv.stack().reindex(emp_dlog.index)\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(np.log(emp_rank), emp_dlog, s=1, alpha=0.05, label=\"Empirical\")\n",
    "ax.set_xlabel(\"log(rank)\"); ax.set_ylabel(\"Δ log share\")\n",
    "ax.set_title(\"Funnel of churn – empirical (daily)\")\n",
    "plt.tight_layout(); fig.savefig(FIG_DIR/\"funnel_empirical.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1bcfe",
   "metadata": {},
   "source": [
    "*Repeat diagnostic plots and survival‑prob tables as per prompt…*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ad4fe",
   "metadata": {},
   "source": [
    "## 8 | Conclusions / Next Steps\n",
    "\n",
    "* Summarise parameter values and goodness‑of‑fit metrics.\n",
    "* Note whether a jump component seems necessary (left‑tail diagnostics).\n",
    "* Outline work to port model to Reddit and to extend list length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6533e",
   "metadata": {},
   "source": [
    "### Provenance\n",
    "Data source: CrowdTangle export of top‑2 000 U.S. Facebook pages, 2023‑01‑01 → 2024‑01‑31.  \n",
    "Notebook autogenerated by OpenAI o3 assistant (conversation ID …).  \n",
    "Run date: {{ cookiecutter.date }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
