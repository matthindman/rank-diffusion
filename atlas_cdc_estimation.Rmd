---
title: 'Rank Diffusion on FB Pages: Stochastic Portfolio Theory-style Model'
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    number_sections: true
params:
  data_path: "./fb_top2000_ranked_daily.parquet"
  horizons:
  - 1
  - 7
  - 28
  ranks_keep:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  - 256
  - 512
  - 1024
  ranks_overlay:
  - 2
  - 8
  - 32
  - 128
  - 512
  top_n_anchor: 1000
  anchor_days:
  - 1
  - 8
  - 15
  - 22
  m_head: 10
  index_top_n: null
  min_n_per_rank: 10
  quantiles:
  - 0.1
  - 0.25
  - 0.5
  - 0.75
  - 0.9
  k_daily:
  - 100
  - 200
  - 300
  - 400
  - 500
  - 600
  - 700
  - 800
  - 900
  weekly_quantiles:
  - 0.1
  - 0.5
  - 0.9
  seed: 1823
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4.6, dpi = 150)
set.seed(params$seed)
```

# 0) Purpose and scope

This notebook implements the full pipeline to estimate a Capital Distribution Curve (CDC) from weekly ranked activity using an Atlas‑style stochastic portfolio theory (SPT) model, strictly following the provided technical design. It uses only the top 1000 ranks for head estimation, fits a constant variance, estimates a monotone rank‑drift schedule, extrapolates a softening tail, enforces stationarity by zero‑sum drift with bottom uplift, converts drifts to exponential gap rates and CDC increments, builds certainty‑equivalent (MCE) weights, and produces diagnostics and a final CDC plot.

Key data expectations (tables might be provided as in‑memory data frames or CSVs under `data/`):

- weekly: `week_start`, `endpoint_id`, `weekly_metric`, `weekly_total`, `weekly_share`, `rank_w`.
- daily (optional): `date`, `endpoint_id`, `metric_value`, `rank`, `total_metric`, `share`.

All growth computations are on shares, not counts.


# 1) Packages and configuration

```{r packages}
required_pkgs <- c(
  "tidyverse",   # data manipulation + ggplot2
  "isotone",     # isotonic regression (PAVA)
  "Iso",         # alternative PAVA implementation
  "robustbase",  # robust stats (Huber)
  "car",         # Levene/Brown–Forsythe test
  "boot",        # bootstrapping support
  "scales",      # plotting scales
  "arrow",       # parquet/feather IO
  "lubridate",   # time handling
  "broom",       # tidying
  "here",        # paths
  "purrr"        # functional programming
)

missing <- setdiff(required_pkgs, rownames(installed.packages()))
if (length(missing) > 0) {
  warning("Missing packages: ", paste(missing, collapse = ", "),
          ". Please install them before running the full pipeline.")
}

invisible(lapply(intersect(required_pkgs, rownames(installed.packages())),
                 library, character.only = TRUE))

# Optional packages for advanced smoothing/fitting and multi-plot layout
opt_pkgs <- c("genlasso", "minpack.lm", "patchwork")
opt_missing <- setdiff(opt_pkgs, rownames(installed.packages()))
invisible(lapply(intersect(opt_pkgs, rownames(installed.packages())),
                 library, character.only = TRUE))
```

```{r config}
config <- list(
  k_max = 1000L,                   # use top 1:1000 ranks (head)
  micro_share_threshold = 1e-8,    # exclude micro-shares to avoid log underflow
  winsor_q = 0.99,                 # winsorize |growth| at 99th pct per-rank
  k0 = 500L,                       # splice rank for tail extrapolation [300,600]
  tail_family = "auto",            # "power", "log", or "auto" for CV selection
  tv_lambda = 0.0,                 # small TV penalty equivalent (see notes)
  N_grid = c(5000L, 10000L, 20000L, 50000L, 100000L, 200000L),
  uplift_m_grid = c(100L, 500L, 1000L, 5000L, 10000L, 20000L),
  do_cv = FALSE,                   # time-blocked CV for tail family
  cv_blocks = 5L,                  # number of time blocks if do_cv = TRUE
  save_outputs = FALSE,            # whether to write tables/figures
  output_dir = "website/plots"     # target dir for figures if saving
)

# Align head size with header parameters when provided
if (!is.null(params$top_n_anchor)) {
  config$k_max <- as.integer(params$top_n_anchor)
}
```


# 2) Helpers

```{r helpers}
# Simple weighted PAVA for nondecreasing fit (Pool Adjacent Violators)
pava_increasing <- function(y, w = NULL) {
  n <- length(y)
  if (n == 0) return(numeric(0))
  if (is.null(w)) w <- rep(1, n)
  w <- ifelse(is.finite(w) & w > 0, w, 1)
  m <- as.numeric(y)
  ww <- as.numeric(w)
  start <- seq_len(n)
  end <- seq_len(n)
  k <- n
  i <- 1
  while (i < k) {
    if (m[i] <= m[i + 1]) {
      i <- i + 1
    } else {
      total_w <- ww[i] + ww[i + 1]
      total_m <- (ww[i] * m[i] + ww[i + 1] * m[i + 1]) / total_w
      m[i] <- total_m
      ww[i] <- total_w
      end[i] <- end[i + 1]
      if (i + 1 < k) {
        m[(i + 1):(k - 1)] <- m[(i + 2):k]
        ww[(i + 1):(k - 1)] <- ww[(i + 2):k]
        start[(i + 1):(k - 1)] <- start[(i + 2):k]
        end[(i + 1):(k - 1)] <- end[(i + 2):k]
      }
      k <- k - 1
      if (i > 1) i <- i - 1
    }
  }
  fit <- numeric(n)
  for (j in seq_len(k)) {
    fit[start[j]:end[j]] <- m[j]
  }
  fit
}
# From user-provided loading utilities
req_cols <- c("date","endpoint_id","metric_value")

read_any <- function(path) {
  ext <- tolower(tools::file_ext(path))
  if (ext %in% c("parquet","feather")) return(arrow::read_parquet(path))
  if (ext %in% c("csv","tsv")) return(readr::read_delim(path, delim = ifelse(ext=="csv", ",", "\t"), show_col_types = FALSE))
  stop("Unsupported file extension: ", ext)
}

safe_rank1 <- function(x, decreasing = TRUE) base::rank(if (decreasing) -x else x, ties.method = "first")
log_brks <- function(rmax) { b <- 2^(0:floor(log2(rmax))); b[b <= rmax] }

# Utility: ensure date
to_date <- function(x) {
  if (inherits(x, "Date")) return(x)
  as.Date(x)
}

# Weighted trimmed mean (one-sided symmetric trim by weight mass)
weighted_trim_mean <- function(x, w, trim = 0.10) {
  stopifnot(length(x) == length(w))
  o <- order(x)
  x <- x[o]; w <- w[o]
  w <- w / sum(w)
  cw <- cumsum(w)
  keep <- (cw >= trim) & (cw <= 1 - trim)
  if (!any(keep)) {
    # fallback: return weighted mean
    return(sum(x * w))
  }
  # Linear interpolation at edges for exact trim mass
  idx <- which(keep)
  xk <- x[idx]; wk <- w[idx]
  # add contribution from partial edge mass if needed
  lower_i <- max(0, max(which(cw < trim)))
  upper_i <- min(length(cw) + 1, min(which(cw > 1 - trim)))
  # Lower edge interpolation
  if (lower_i > 0 && lower_i < length(cw)) {
    wt_needed <- trim - cw[lower_i]
    if (wt_needed > 0) {
      x_val <- x[lower_i + 1]
      xk <- c(x_val, xk)
      wk <- c(wt_needed, wk)
    }
  }
  # Upper edge interpolation
  if (upper_i > 1 && upper_i <= length(cw)) {
    wt_needed <- cw[upper_i] - (1 - trim)
    if (wt_needed > 0) {
      x_val <- x[upper_i]
      xk <- c(xk, x_val)
      wk <- c(wk, wt_needed)
    }
  }
  wk <- wk / sum(wk)
  sum(xk * wk)
}

# Winsorize by rank: cap |r| at rank-wise 99th percentile of |r|
winsorize_by_rank <- function(df, rank_col = "rank_w", value_col = "r_f", q = 0.99) {
  df %>%
    group_by(.data[[rank_col]]) %>%
    mutate(
      .abs = abs(.data[[value_col]]),
      .thr = quantile(.abs, probs = q, na.rm = TRUE, type = 7),
      !!rlang::sym(value_col) := pmax(pmin(.data[[value_col]], .thr), -.thr)
    ) %>%
    ungroup() %>%
    select(-.abs, -.thr)
}

# Contiguous rank check for 1:K within a week
has_contiguous_head <- function(df, k_max) {
  rks <- sort(unique(df$rank_w))
  all(1:k_max %in% rks)
}

# Drop dates with sparse rank coverage so downstream growth pairs stay stable
filter_low_rank_dates <- function(daily_tbl, min_ranks = 1850) {
  counts <- daily_tbl %>% count(date, name = "n")
  sparse_dates <- counts %>% filter(n < min_ranks)
  list(
    daily = daily_tbl %>% anti_join(sparse_dates, by = "date"),
    removed_dates = sparse_dates
  )
}

# Synthetic weekly data generator for demo runs
simulate_weekly_data <- NULL  # disabled per user instruction (no synthetic data)

# Build occupant forward growth panel restricted to top k_max
build_forward_growth <- function(weekly, k_max = 1000L, micro_share_threshold = 1e-8, winsor_q = 0.99) {
  wk <- weekly %>%
    mutate(week_start = to_date(week_start)) %>%
    filter(!is.na(weekly_share), weekly_share > 0, rank_w <= k_max) %>%
    mutate(log_share = log(weekly_share)) %>%
    select(week_start, endpoint_id, rank_w, weekly_share, log_share, weekly_metric, weekly_total)

  # Drop weeks missing any rank in 1:k_max
  kept_weeks <- wk %>%
    group_by(week_start) %>%
    filter(n() >= k_max) %>%
    summarise(contig = has_contiguous_head(cur_data(), k_max), .groups = "drop") %>%
    filter(contig) %>%
    pull(week_start)

  wk <- wk %>% filter(week_start %in% kept_weeks)

  # Week index for lead join
  weeks <- sort(unique(wk$week_start))
  wmap <- tibble(week_start = weeks, week_id = seq_along(weeks))
  wk <- wk %>% left_join(wmap, by = "week_start")

  wk_next <- wk %>%
    mutate(week_id = week_id - 1L) %>%   # so join by week_id gives t+1 on the right
    select(endpoint_id, week_id, log_share_next = log_share)

  gpanel <- wk %>%
    left_join(wk_next, by = c("endpoint_id", "week_id")) %>%
    filter(!is.na(log_share_next), weekly_share >= micro_share_threshold) %>%
    mutate(r_f = log_share_next - log_share)

  # Winsorize forward growth |r_f| at rank level
  gpanel <- winsorize_by_rank(gpanel, rank_col = "rank_w", value_col = "r_f", q = winsor_q)

  gpanel
}

# Summaries by rank: mean, median, variance, n
summarize_growth_by_rank <- function(gpanel, k_max = 1000L) {
  gsum <- gpanel %>%
    filter(rank_w <= k_max) %>%
    group_by(rank_w) %>%
    summarise(
      bar_g_k = mean(r_f, na.rm = TRUE),
      tilde_g_k = median(r_f, na.rm = TRUE),
      v_k = var(r_f, na.rm = TRUE),
      n_k = n(),
      .groups = "drop"
    )
  gsum
}

# Adjacent head gaps and empirical CDC head
compute_head_gaps <- function(weekly, k_max = 1000L) {
  wk <- weekly %>%
    mutate(week_start = to_date(week_start)) %>%
    filter(!is.na(weekly_share), weekly_share > 0, rank_w <= k_max) %>%
    mutate(log_share = log(weekly_share)) %>%
    select(week_start, rank_w, weekly_share, log_share)

  # Keep weeks with contiguous head
  kept_weeks <- wk %>%
    group_by(week_start) %>%
    filter(n() >= k_max) %>%
    summarise(contig = has_contiguous_head(cur_data(), k_max), .groups = "drop") %>%
    filter(contig) %>% pull(week_start)

  wk <- wk %>% filter(week_start %in% kept_weeks)

  deltas <- wk %>%
    arrange(week_start, rank_w) %>%
    group_by(week_start) %>%
    mutate(delta = log_share - dplyr::lead(log_share)) %>%
    ungroup() %>%
    filter(rank_w < k_max)

  delta_bar <- deltas %>%
    group_by(rank_w) %>%
    summarise(overline_delta_k = mean(delta, na.rm = TRUE), .groups = "drop")

  mhat <- wk %>%
    group_by(rank_w) %>%
    summarise(mhat_k = mean(log_share, na.rm = TRUE), .groups = "drop")

  list(delta_bar = delta_bar, mhat = mhat)
}

# Pool constant variance sigma^2 via weighted trimmed mean of v_k
pool_sigma2 <- function(gsum, trim = 0.10) {
  df <- gsum %>% filter(is.finite(v_k), is.finite(n_k), n_k > 1)
  if (nrow(df) == 0) stop("No finite per-rank variances available for pooling.")
  sigma2_hat <- weighted_trim_mean(x = df$v_k, w = df$n_k, trim = trim)
  as.numeric(sigma2_hat)
}

# Brown–Forsythe (median-centered Levene) across rank deciles
levene_bf_test <- function(gpanel, k_max = 1000L) {
  if (!requireNamespace("car", quietly = TRUE)) return(NULL)
  df <- gpanel %>%
    filter(rank_w <= k_max) %>%
    mutate(decile = cut(rank_w, breaks = seq(0, k_max, by = k_max/10), include.lowest = TRUE, labels = FALSE))
  car::leveneTest(r_f ~ as.factor(decile), data = df, center = median)
}

# Isotonic regression on bar_g_k with weights; optional TV-like smoothing
fit_isotonic_drift <- function(gsum, tv_lambda = 0.0) {
  # Keep only ranks with finite mean growth and positive weights
  df <- gsum %>% filter(is.finite(bar_g_k), is.finite(n_k), n_k > 0) %>% arrange(rank_w)
  x <- df$rank_w; y <- df$bar_g_k; w <- df$n_k
  if (length(x) == 0) stop("No finite rank-wise means available for isotonic drift fit.")

  # Primary isotonic fit via in-notebook weighted PAVA
  g_iso <- pava_increasing(y, w)
  if (length(g_iso) != length(y)) {
    # Fallback to unweighted isoreg to enforce size and monotonicity
    iso <- stats::isoreg(seq_along(y), y)
    g_iso <- cummax(as.numeric(iso$yf))
  }
  if (length(g_iso) != length(y)) {
    # Last-resort size fix using cumulative max of raw y
    g_iso <- cummax(y)
  }
  if (length(g_iso) != length(y)) {
    # Final guard: constant fill
    g_iso <- rep_len(mean(y, na.rm = TRUE), length(y))
  }

  if (length(g_iso) != length(x)) {
    g_iso <- rep_len(mean(y, na.rm = TRUE), length(x))
  }

  # Optional: TV-like smoothing; re-project to isotonic using the same PAVA
  if (tv_lambda > 0 && requireNamespace("genlasso", quietly = TRUE)) {
    y_w <- sqrt(w) * y
    X_w <- diag(sqrt(w))
    D <- diff(diag(length(y)))
    fl <- genlasso::genlasso(y = y_w, X = X_w, D = D)
    lam_max <- max(fl$lambda)
    lam <- min(tv_lambda * lam_max, lam_max)
    g_smooth <- as.numeric(predict(fl, lambda = lam)$fit)
    g_iso <- pava_increasing(g_smooth, w)
    if (length(g_iso) != length(y)) {
      iso2 <- stats::isoreg(seq_along(g_smooth), g_smooth)
      g_iso <- cummax(as.numeric(iso2$yf))
    }
  }

  tibble(rank_w = x, gk_hat = g_iso)
}

# Gap-based cross-check of drifts using overline_delta_k and sigma2
drift_from_gaps <- function(delta_bar, sigma2_hat) {
  stopifnot("overline_delta_k" %in% names(delta_bar))
  # r_k = 1 / overline_delta_k
  r_k <- 1 / pmax(delta_bar$overline_delta_k, .Machine$double.eps)
  Gk <- - (sigma2_hat / 2) * r_k
  gk_gap <- c(Gk[1], diff(Gk))
  tibble(rank_w = delta_bar$rank_w, gk_gap = gk_gap, r_k_gap = r_k, Gk_gap = Gk)
}

# Tail families (anchored at k0):
# power: g_k = -a * (k/k0)^(-alpha) with a = -g(k0), alpha>0
fit_tail_power <- function(k, gk, k0, w) {
  # Fit a>0, alpha>0 via WLS objective
  fn <- function(theta) {
    a <- exp(theta[1]); alpha <- exp(theta[2])
    pred <- -a * (k / k0)^(-alpha)
    sum(w * (gk - pred)^2)
  }
  opt <- optim(par = c(log(0.01), log(0.5)), fn = fn, method = "BFGS")
  a_hat <- exp(opt$par[1]); alpha_hat <- exp(opt$par[2])
  list(family = "power", a = a_hat, alpha = alpha_hat, c = NA_real_)
}

# log-decay: g_k = -a / (1 + c * log(k/k0)) with a = -g(k0), c>0
fit_tail_log <- function(k, gk, k0, w) {
  # Fit a>0, c>0 via WLS objective
  fn <- function(theta) {
    a <- exp(theta[1]); cpos <- exp(theta[2])
    pred <- -a / (1 + cpos * log(pmax(k / k0, 1)))
    sum(w * (gk - pred)^2)
  }
  opt <- optim(par = c(log(0.01), log(0.5)), fn = fn, method = "BFGS")
  a_hat <- exp(opt$par[1]); c_hat <- exp(opt$par[2])
  list(family = "log", a = a_hat, alpha = NA_real_, c = c_hat)
}

predict_tail <- function(par, k, k0) {
  if (par$family == "power") {
    -par$a * (k / k0)^(-par$alpha)
  } else {
    -par$a / (1 + par$c * log(pmax(k / k0, 1)))
  }
}

# Build full g_k up to N using head estimate and tail params
build_g_schedule <- function(g_head, par, N, k0) {
  k_head <- g_head$rank_w
  gk_head <- g_head$gk_hat
  k_max_head <- max(k_head)
  if (k_max_head < k0) stop("Head drift estimates do not reach k0; lower k0 or ensure coverage.")
  k_all <- 1:N
  g_all <- numeric(N)
  # Densify head 1:k_max_head by linear interpolation (isotonic steps are piecewise constant)
  dens <- approx(x = k_head, y = gk_head, xout = 1:k_max_head, method = "linear", rule = 2)$y
  g_all[1:k_max_head] <- dens
  if (N > k_max_head) {
    k_tail <- (k_max_head + 1):N
    g_all[k_tail] <- predict_tail(par, k_tail, k0 = k0)
  }
  # Ensure monotone nondecreasing across all k
  g_all <- pmax(g_all, cummax(g_all))
  tibble(rank_w = k_all, gk = g_all)
}

# Enforce stationarity via global centering + bottom uplift on last m ranks
enforce_stationarity <- function(g_sched, m, verbose = FALSE) {
  N <- nrow(g_sched)
  if (N <= 1L) return(list(gk = g_sched$gk, uplift = 0, ok = TRUE, reason = "trivial"))
  if (m >= N) m <- N - 1L
  g <- g_sched$gk
  G0 <- cumsum(g)
  S0 <- sum(g)
  tol <- 1e-12

  k <- seq_len(N - 1L)
  t_k <- pmax(0, k - (N - m))
  A_k <- k - (N / m) * t_k
  # Numerical guard: ensure A_k > 0
  A_k[A_k <= 0] <- min(A_k[A_k > 0], na.rm = TRUE)
  # Lower bound on c_shift that ensures G2[k] <= -tol for all k after tail adjustment
  LB_k <- (tol + G0[k] - (S0 * t_k) / m) / A_k
  c_shift <- max(0, suppressWarnings(max(LB_k[is.finite(LB_k)], na.rm = TRUE))) + 1e-12

  # Apply shift and compute tail uplift to enforce zero-sum
  g1 <- g - c_shift
  S1 <- sum(g1)
  u <- (-S1) / m
  g2 <- g1
  g2[(N - m + 1):N] <- g2[(N - m + 1):N] + u

  G <- cumsum(g2)
  ok <- all(G[1:(N - 1)] < -tol) && abs(sum(g2)) <= 1e-9
  if (!ok && verbose) {
    max_prefix <- max(G[1:(N - 1)])
    message(sprintf("Stationarity check failed: max prefix = %.3e, sum(g) = %.3e, c_shift = %.3e, u = %.3e", max_prefix, sum(g2), c_shift, u))
  }
  if (!ok) return(list(gk = g2, uplift = u, ok = FALSE, reason = "partial_sum_violation"))
  list(gk = g2, uplift = u, ok = TRUE, reason = "ok")
}

# From drifts to gaps to CDC and MCE
drifts_to_cdc_mce <- function(gk, sigma2_hat, anchor_m1) {
  N <- length(gk)
  Gk <- cumsum(gk)
  r_k <- -2 * Gk[1:(N - 1)] / sigma2_hat
  if (any(r_k <= 0 | !is.finite(r_k))) {
    stop("Nonpositive or invalid r_k encountered; check drift schedule and sigma^2.")
  }
  rho_k <- 1 / r_k
  # CDC increments: m(k+1) - m(k) = -rho_k
  m <- numeric(N)
  m[1] <- anchor_m1
  for (k in 1:(N - 1)) m[k + 1] <- m[k] - rho_k[k]
  # MCE weights
  S <- rev(cumsum(rev(rho_k)))
  S <- c(S, 0)  # S_N = 0
  w_star <- exp(S)
  mce <- w_star / sum(w_star)
  list(Gk = Gk, r_k = r_k, rho_k = rho_k, m = m, mce = mce)
}

# Score head fit and share discrepancy
score_candidate <- function(m_model, m_obs, k_max, mce, s_obs_1000 = NA_real_) {
  # Align by ranks 1:k_max
  y_true <- m_obs$mhat_k[1:k_max]
  y_hat <- m_model[1:k_max]
  rmse <- sqrt(mean((y_true - y_hat)^2, na.rm = TRUE))
  s_mod <- sum(mce[1:k_max])
  share_err <- if (is.na(s_obs_1000)) NA_real_ else abs(s_mod - s_obs_1000)
  list(rmse = rmse, s_mod_1000 = s_mod, share_err = share_err)
}
```


# 3) Data ingest

This notebook expects a file path parameter `params$data_path` pointing to a daily interactions dataset (parquet/csv/tsv). The loader reads the file, computes daily shares and ranks, and then builds weekly aggregates used by the Atlas model.

```{r ingest}
stopifnot(!is.null(params$data_path))

# Resolve data_path robustly: expand ~, try as-given, and here::here()
p_in <- as.character(params$data_path)
p_expanded <- path.expand(p_in)
p_here <- tryCatch(here::here(p_in), error = function(e) NA_character_)
bn <- basename(p_in)
# Also try common data locations
p_data <- tryCatch(here::here("data", bn), error = function(e) NA_character_)
p_webdata <- tryCatch(here::here("website", "data", bn), error = function(e) NA_character_)
candidates <- unique(na.omit(c(p_expanded, p_here, p_data, p_webdata)))
existing <- candidates[file.exists(candidates)]
if (length(existing) == 0) {
  stop(sprintf(
    "File not found. Tried: %s. getwd() = %s",
    paste(candidates, collapse = " | "), getwd()
  ))
}
data_path_resolved <- existing[[1]]

raw <- read_any(data_path_resolved) %>% tibble::as_tibble()
stopifnot(all(req_cols %in% names(raw)))

daily <- raw %>%
  mutate(
    date = as.Date(date),
    endpoint_id = as.character(endpoint_id)
  ) %>%
  filter(metric_value > 0) %>%
  group_by(date) %>%
  mutate(
    total_metric = sum(metric_value, na.rm = TRUE),
    share        = metric_value / total_metric
  ) %>%
  ungroup()

if (!"rank" %in% names(daily)) {
  daily <- daily %>%
    group_by(date) %>%
    mutate(rank = as.integer(safe_rank1(share, decreasing = TRUE))) %>%
    ungroup()
}

# Drop dates with sparse rank coverage so downstream growth pairs stay stable
filter_result <- filter_low_rank_dates(daily, min_ranks = 1850)
daily <- filter_result$daily
removed_low_rank_dates <- filter_result$removed_dates

if (nrow(removed_low_rank_dates) > 0) {
  message(
    sprintf(
      "Filtered %d dates with < %d ranks (min = %d).",
      nrow(removed_low_rank_dates),
      1850,
      min(removed_low_rank_dates$n)
    )
  )
}

# Diagnostics
dup_ct <- daily %>% count(date, endpoint_id) %>% filter(n>1) %>% nrow()
day_counts <- daily %>% count(date)
diag_tbl <- tibble::tibble(
  rows = nrow(daily),
  days = dplyr::n_distinct(daily$date),
  unique_pages = dplyr::n_distinct(daily$endpoint_id),
  min_pages_per_day = min(day_counts$n),
  median_pages_per_day = median(day_counts$n),
  max_pages_per_day = max(day_counts$n),
  duplicated_pairs = dup_ct
)
knitr::kable(diag_tbl, caption = "Dataset diagnostics")

ggplot(daily, aes(x = share)) +
  geom_histogram(bins = 80) +
  scale_x_log10(labels = scales::percent_format(accuracy = 0.01)) +
  labs(title = "Share distribution", x = "Share (log)", y = "Count")
```

```{r weekly_build}
# Weekly aggregation
weekly <- daily %>%
  mutate(week_start = lubridate::floor_date(date, unit = "week", week_start = 1)) %>%  # Monday
  group_by(week_start, endpoint_id) %>%
  summarise(weekly_metric = sum(metric_value, na.rm = TRUE), .groups = "drop") %>%
  group_by(week_start) %>%
  mutate(
    weekly_total = sum(weekly_metric, na.rm = TRUE),
    weekly_share = weekly_metric / weekly_total,
    rank_w = as.integer(rank(-weekly_share, ties.method = "first"))
  ) %>%
  ungroup()

# Weekly (t, t+H) pairs helper for diagnostics/sensitivity (H=1 default)
build_weekly_pairs_observed <- function(weekly, H = 1, max_rank = Inf) {
  w_all   <- sort(unique(weekly$week_start))
  valid_t <- w_all[(w_all + lubridate::weeks(H)) %in% w_all]

  base <- weekly %>%
    filter(week_start %in% valid_t, rank_w <= max_rank) %>%
    transmute(week_start, endpoint_id,
              rank_t = rank_w,
              a_t    = weekly_metric,
              w_t    = weekly_share)

  later <- weekly %>%
    transmute(week_start = week_start - lubridate::weeks(H), endpoint_id,
              a_tpH   = weekly_metric,
              w_tpH   = weekly_share,
              rank_tpH = rank_w)

  base %>%
    inner_join(later, by = c("week_start","endpoint_id")) %>%
    mutate(horizon = H,
           g       = log(a_tpH) - log(a_t),
           dlog    = log(w_tpH) - log(w_t))
}
```


# 4) Descriptives for head (ranks 1:1000)

```{r descriptives}
# A) Rank-occupant forward growth panel
gpanel <- build_forward_growth(
  weekly,
  k_max = config$k_max,
  micro_share_threshold = config$micro_share_threshold,
  winsor_q = config$winsor_q
)

# B) Summaries by rank
gsum <- summarize_growth_by_rank(gpanel, k_max = config$k_max)

# C) Adjacent head gaps and empirical CDC head
head_gaps <- compute_head_gaps(weekly, k_max = config$k_max)
delta_bar <- head_gaps$delta_bar
mhat <- head_gaps$mhat

# Convenience merged frame
head_summary <- gsum %>%
  left_join(delta_bar, by = "rank_w") %>%
  left_join(mhat, by = "rank_w")

head(head_summary)
```

```{r plots_descriptives, fig.height=4.8}
p1 <- ggplot(head_summary, aes(x = rank_w)) +
  geom_line(aes(y = bar_g_k, color = "mean"), linewidth = 0.6) +
  geom_line(aes(y = tilde_g_k, color = "median"), linewidth = 0.6, linetype = 2) +
  scale_color_manual(values = c(mean = "#1b9e77", median = "#d95f02"), name = "stat") +
  labs(title = "Rank-conditional forward log-growth", x = "rank k", y = "growth (forward)") +
  theme_minimal()

p2 <- ggplot(head_summary, aes(x = rank_w, y = v_k)) +
  geom_line(color = "#7570b3", linewidth = 0.6) +
  labs(title = "Rank-wise variance of forward growth", x = "rank k", y = "variance v_k") +
  theme_minimal()

p3 <- ggplot(head_summary, aes(x = rank_w, y = overline_delta_k)) +
  geom_line(color = "#666666", linewidth = 0.6) +
  labs(title = "Average adjacent log-gap over weeks", x = "rank k", y = "overline d_k") +
  theme_minimal()

if (requireNamespace("patchwork", quietly = TRUE)) {
  patchwork::wrap_plots(p1, p2, p3, ncol = 1)
} else {
  print(p1); print(p2); print(p3)
}
```

```{r cdc_head_plot, fig.height=4.5}
# CDC head: k vs mhat(k) on log–log axes (plot exp(mhat) vs rank on log-log)
cdc_head_level <- mhat %>% mutate(level = exp(mhat_k))
ggplot(cdc_head_level, aes(x = rank_w, y = level)) +
  geom_line(color = "black", linewidth = 0.8) +
  scale_x_log10(labels = scales::comma_format()) +
  scale_y_log10() +
  labs(title = "CDC head (geometric mean level)", x = "rank k (log)", y = "geometric mean share (log)") +
  theme_minimal()
```


# 5) Constant variance estimate and variance flatness test

```{r variance}
sigma2_hat <- pool_sigma2(gsum, trim = 0.10)
sigma2_hat
```

```{r variance_test}
levene_bf <- tryCatch(levene_bf_test(gpanel, k_max = config$k_max), error = function(e) NULL)
if (!is.null(levene_bf)) print(levene_bf)
```

Addendum: Unless the Brown–Forsythe test strongly rejects variance flatness across rank deciles, we proceed with a constant `sigma2_hat` for the head.


# 6) Rank‑drift estimation on 1:1000

Primary approach (A): fit a monotone nondecreasing schedule `g_k` to `bar_g_k` with weights `n_k` via isotonic regression (PAVA). A small TV penalty is optionally approximated by fused‑lasso smoothing followed by an isotonic projection when `genlasso` is available and `tv_lambda > 0`. Cross‑check (B): use average adjacent gaps to derive `g_k` via the exponential gap rate identity using `sigma2_hat`.

```{r drift_fit}
gk_iso <- fit_isotonic_drift(gsum, tv_lambda = config$tv_lambda)
gk_gap <- drift_from_gaps(delta_bar, sigma2_hat = sigma2_hat)

# Merge for comparison
gk_compare <- gk_iso %>% left_join(gk_gap, by = "rank_w")

ggplot(gk_compare, aes(x = rank_w)) +
  geom_line(aes(y = gk_hat, color = "isotonic"), linewidth = 0.6) +
  geom_line(aes(y = gk_gap, color = "gap"), linewidth = 0.6, linetype = 2) +
  scale_color_manual(values = c(isotonic = "#1b9e77", gap = "#d95f02"), name = "drift") +
  labs(title = "Drift schedule: Approach A (isotonic) vs B (gap)", x = "rank k", y = "g_k") +
  theme_minimal()
```

If large discrepancies appear between approaches, revisit variance flatness, outliers, and week selection (winsorization threshold, micro‑share filter). Approach A remains primary for subsequent steps.


# 7) Tail extrapolation (k > 1000)

We splice a monotone softening family at `k0  [300, 600]` and fit with weighted nonlinear least squares over ranks `k0:1000`.

```{r tail_fit}
k0 <- as.integer(config$k0)
stopifnot(k0 >= 300L, k0 <= 600L)

seg <- gk_iso %>% filter(rank_w >= k0, rank_w <= config$k_max) %>%
  left_join(gsum %>% select(rank_w, n_k), by = "rank_w")

if (nrow(seg) < 5) {
  # Fallback: use last ~200 available head ranks if splice window is empty
  rk_max_avail <- max(gk_iso$rank_w)
  seg <- gk_iso %>% filter(rank_w >= max(1L, rk_max_avail - 200L)) %>%
    left_join(gsum %>% select(rank_w, n_k), by = "rank_w")
}
seg$n_k[is.na(seg$n_k)] <- 1

par_power <- fit_tail_power(k = seg$rank_w, gk = seg$gk_hat, k0 = k0, w = seg$n_k)
par_log <- fit_tail_log(k = seg$rank_w, gk = seg$gk_hat, k0 = k0, w = seg$n_k)

# Simple selection without CV unless enabled: compare WSS on segment
wss_power <- sum(seg$n_k * (seg$gk_hat - predict_tail(par_power, seg$rank_w, k0))^2)
wss_log <- sum(seg$n_k * (seg$gk_hat - predict_tail(par_log, seg$rank_w, k0))^2)

if (isTRUE(config$do_cv)) {
  # Time-blocked CV placeholder; by default disabled for speed.
  # Implementing full re‑estimation per block is available but off by default.
}

tail_par <- if (identical(config$tail_family, "power")) par_power else if (identical(config$tail_family, "log")) par_log else if (wss_power <= wss_log) par_power else par_log
tail_par
```


# 8) Stationarity and effective population size N

We extend `g_k` beyond 1000 using the selected tail family to candidate `N` values. For each `N`, if `sum_{k=1}^N g_k < 0`, we add a bottom uplift (a small positive constant drift over the last `m` ranks) so that `sum g_k = 0` and `G(k) < 0` for all `k < N`. We score `(N, m)` by CDC head fit and top‑1000 share fit.

```{r stationarity, message=TRUE, warning=TRUE}
# Observed top-1000 share (average over weeks) if weekly_total is available
obs_top1000_share <- NA_real_
if (all(c("weekly_metric", "weekly_total") %in% names(weekly))) {
  top1000_by_week <- weekly %>%
    filter(rank_w <= config$k_max) %>%
    group_by(week_start) %>%
    summarise(A1000 = sum(weekly_metric, na.rm = TRUE), total = max(weekly_total, na.rm = TRUE), .groups = "drop") %>%
    filter(is.finite(A1000), is.finite(total), total > 0)
  if (nrow(top1000_by_week) > 0) {
    obs_top1000_share <- mean(top1000_by_week$A1000 / top1000_by_week$total, na.rm = TRUE)
  }
}

best <- NULL
best_model <- NULL
m1_anchor <- mhat$mhat_k[mhat$rank_w == 1]

# Diagnostics: tail parameters and base schedule summaries
message("[stationarity] Diagnostics starting…")
message(sprintf("sigma2_hat = %.6e; k0 = %d", sigma2_hat, k0))
if (!is.null(tail_par$family)) message(sprintf("tail_family = %s", tail_par$family))
message(sprintf("N_grid: %s", paste(config$N_grid, collapse = ", ")))
message(sprintf("uplift_m_grid: %s", paste(config$uplift_m_grid, collapse = ", ")))

base_diag <- lapply(config$N_grid, function(N) {
  gs <- build_g_schedule(g_head = gk_iso, par = tail_par, N = N, k0 = k0)
  g <- gs$gk
  G0 <- cumsum(g)
  data.frame(N = N,
             sum_g = sum(g),
             max_prefix = if (N > 1) max(G0[1:(N - 1)]) else NA_real_,
             min_g = min(g), max_g = max(g),
             g1 = g[1], gN = g[N])
}) %>% dplyr::bind_rows()
print(utils::head(base_diag, 10))

fail_log <- dplyr::tibble(N = integer(), m = integer(), reason = character())

for (N in config$N_grid) {
  g_sched <- build_g_schedule(g_head = gk_iso, par = tail_par, N = N, k0 = k0)
  for (m in config$uplift_m_grid) {
    enf <- enforce_stationarity(g_sched, m = m, verbose = TRUE)
    if (!isTRUE(enf$ok)) {
      if (nrow(fail_log) < 50) fail_log <- dplyr::bind_rows(fail_log, dplyr::tibble(N = N, m = m, reason = enf$reason))
      next
    }
    out <- drifts_to_cdc_mce(gk = enf$gk, sigma2_hat = sigma2_hat, anchor_m1 = m1_anchor)
    sc <- score_candidate(m_model = out$m, m_obs = mhat, k_max = config$k_max, mce = out$mce, s_obs_1000 = obs_top1000_share)
    item <- list(N = N, m = m, uplift = enf$uplift, rmse = sc$rmse, share_err = sc$share_err, s_mod_1000 = sc$s_mod_1000, gk = enf$gk, m = out$m, mce = out$mce, r_k = out$r_k, rho_k = out$rho_k)
    if (is.null(best)) {
      best <- item
    } else {
      better <- (item$rmse < best$rmse - 1e-12) || (abs(item$rmse - best$rmse) <= 1e-12 && (is.na(item$share_err) || item$share_err < best$share_err))
      if (better) best <- item
    }
  }
}

# Fallback: if no (N, m) combination passes strict stationarity, construct a
# guaranteed stationary schedule by global centering and last-element adjustment.
if (is.null(best)) {
  if (nrow(fail_log) > 0) {
    message("[stationarity] All (N, m) attempts failed. Sample of failures:")
    print(fail_log)
    message("[stationarity] Failure counts by reason:")
    print(dplyr::count(fail_log, reason))
  }
  N_fb <- max(config$N_grid)
  g_sched <- build_g_schedule(g_head = gk_iso, par = tail_par, N = N_fb, k0 = k0)
  g <- g_sched$gk
  if (length(g) <= 1) stop("Insufficient length of drift schedule to enforce stationarity.")
  G0 <- cumsum(g)
  k <- seq_len(N_fb - 1L)
  # Shift so that all prefixes are strictly negative
  c_shift <- max(0, max(G0[k] / k, na.rm = TRUE)) + 1e-12
  g2 <- g - c_shift
  # Adjust last element to make total sum exactly zero (keeps prefixes unchanged)
  g2[N_fb] <- g2[N_fb] - sum(g2)
  G <- cumsum(g2)
  if (!all(G[1:(N_fb - 1L)] < -1e-12)) {
    warning("Relaxed fallback did not strictly satisfy negative prefixes; proceeding with closest feasible schedule.")
  }
  out <- drifts_to_cdc_mce(gk = g2, sigma2_hat = sigma2_hat, anchor_m1 = m1_anchor)
  sc <- score_candidate(m_model = out$m, m_obs = mhat, k_max = config$k_max, mce = out$mce, s_obs_1000 = obs_top1000_share)
  best <- list(N = N_fb, m = N_fb - 1L, uplift = NA_real_, rmse = sc$rmse, share_err = sc$share_err, s_mod_1000 = sc$s_mod_1000, gk = g2, m = out$m, mce = out$mce, r_k = out$r_k, rho_k = out$rho_k)
}

best$N; best$m; best$uplift; best$rmse; best$share_err; best$s_mod_1000
```


# 9) Final CDC and plots

```{r final_plots, fig.height=4.8}
final_m <- best$m
final_mce <- best$mce
N_sel <- best$N

# Levels for log–log CDC plot
head_level <- mhat %>% mutate(level = exp(mhat_k))
tail_level <- tibble(rank_w = 1:N_sel, level = exp(final_m))

ggplot() +
  geom_line(data = tail_level %>% filter(rank_w > config$k_max), aes(x = rank_w, y = level), color = "#9e9e9e", linewidth = 0.8, linetype = 2) +
  geom_line(data = head_level, aes(x = rank_w, y = level), color = "black", linewidth = 0.9) +
  scale_x_log10(labels = scales::comma_format()) +
  scale_y_log10() +
  labs(title = "Final CDC (head observed, tail modeled)", x = "rank k (log)", y = "weight level (log)") +
  theme_minimal()
```

```{r save_outputs, eval=config$save_outputs}
dir.create(config$output_dir, recursive = TRUE, showWarnings = FALSE)
ggplot2::ggsave(filename = file.path(config$output_dir, "cdc_final.png"), width = 8, height = 5, dpi = 150)

# Save tables
readr::write_csv(tibble(rank_w = 1:best$N, m = best$m, mce = best$mce), file.path(config$output_dir, "cdc_m_mce.csv"))
readr::write_csv(head_summary, file.path(config$output_dir, "head_summary.csv"))
```


# 10) Robustness (optional hooks)

The following hooks align with the design for robustness:

- Variance flatness: inspected via Brown–Forsythe test; sensitivity runs (not shown) can allow decile‑specific `sigma2` in the `drifts_to_cdc_mce` conversion to quantify impact on the CDC head.
- Centered differences: optionally recompute growth as `[log mu_{i,t+1}  - log mu_{i,t-1}]/2` with extended joins.
- Outliers: winsorization and micro‑share filters implemented; Huber loss variant for drift fitting can be added by reweighting `bar_g_k`.
- Time blocking: block bootstrap by weeks (e.g., 6–8‑week blocks) can re‑run steps C–G to produce bands for `g_k`, `r_k`, `m(k)`, and the modeled top‑1000 share.
- Head–tail sensitivity: vary `k0`, change tail family, and grid over `N`; bands can be reported for top‑1000 modeled share and tail mass.


# 11) Edge cases and data hygiene

- Ties: assumed broken deterministically upstream; if not, ensure `rank_w` is stable (e.g., by previous week’s order).
- Missing ranks: weeks with gaps in 1:1000 are dropped by `has_contiguous_head`.
- Exposure filter: growth uses only occupants present at `t` and `t+1`.
- Micro‑shares: excluded below `1e-8`.
- Winsorization: top 1% of `|r_f|` winsorized at the rank level.
- Reproducibility: RNG seed set; session info recorded below.


# 12) Session info

```{r session}
sessionInfo()
```
 # (pava_increasing defined earlier in helpers)
